<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[numpy_100]]></title>
    <url>%2F2018%2F08%2F08%2Fnumpy-100%2F</url>
    <content type="text"><![CDATA[100 numpy exercises一直苦于找不到一个可以练手numpy的地方，有次闲逛github让我如获至宝地发现了它-numpy-100。这是一套基于jupyter notebook的练习题，这套练习对于熟悉numpy非常有帮助，里面的题目也很实用，有兴趣的可以去github上clone下来。 1. Import the numpy package under the name np (★☆☆)1import numpy as np 2. Print the numpy version and the configuration (★☆☆)12print(np.__version__)np.show_config() 3. Create a null vector of size 10 (★☆☆)12Z = np.zeros(10)print(Z) 4. How to find the memory size of any array (★☆☆)12Z = np.zeros((10,10))print("%d bytes" % (Z.size * Z.itemsize)) 5. How to get the documentation of the numpy add function from the command line? (★☆☆)1%run `python -c "import numpy; numpy.info(numpy.add)"` 6. Create a null vector of size 10 but the fifth value which is 1 (★☆☆)123Z = np.zeros(10)Z[4] = 1print(Z) 7. Create a vector with values ranging from 10 to 49 (★☆☆)12Z = np.arange(10,50)print(Z) 8. Reverse a vector (first element becomes last) (★☆☆)123Z = np.arange(50)Z = Z[::-1]print(Z) 9. Create a 3x3 matrix with values ranging from 0 to 8 (★☆☆)12Z = np.arange(9).reshape(3,3)print(Z) 10. Find indices of non-zero elements from [1,2,0,0,4,0] (★☆☆)12nz = np.nonzero([1,2,0,0,4,0])print(nz) 11. Create a 3x3 identity matrix (★☆☆)12Z = np.eye(3)print(Z) 12. Create a 3x3x3 array with random values (★☆☆)12Z = np.random.random((3,3,3))print(Z) 13. Create a 10x10 array with random values and find the minimum and maximum values (★☆☆)123Z = np.random.random((10,10))Zmin, Zmax = Z.min(), Z.max()print(Zmin, Zmax) 14. Create a random vector of size 30 and find the mean value (★☆☆)123Z = np.random.random(30)m = Z.mean()print(m) 15. Create a 2d array with 1 on the border and 0 inside (★☆☆)123Z = np.ones((10,10))Z[1:-1,1:-1] = 0print(Z) 16. How to add a border (filled with 0’s) around an existing array? (★☆☆)123Z = np.ones((5,5))Z = np.pad(Z, pad_width=1, mode='constant', constant_values=0)print(Z) 17. What is the result of the following expression? (★☆☆)12345print(0 * np.nan)print(np.nan == np.nan)print(np.inf &gt; np.nan)print(np.nan - np.nan)print(0.3 == 3 * 0.1) 18. Create a 5x5 matrix with values 1,2,3,4 just below the diagonal (★☆☆)12Z = np.diag(1+np.arange(4),k=-1)print(Z) 19. Create a 8x8 matrix and fill it with a checkerboard pattern (★☆☆)1234Z = np.zeros((8,8),dtype=int)Z[1::2,::2] = 1Z[::2,1::2] = 1print(Z) 20. Consider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?1print(np.unravel_index(99,(6,7,8))) 21. Create a checkerboard 8x8 matrix using the tile function (★☆☆)12Z = np.tile( np.array([[0,1],[1,0]]), (4,4))print(Z) 22. Normalize a 5x5 random matrix (★☆☆)1234Z = np.random.random((5,5))Zmax, Zmin = Z.max(), Z.min()Z = (Z - Zmin)/(Zmax - Zmin)print(Z) 23. Create a custom dtype that describes a color as four unsigned bytes (RGBA) (★☆☆)1234color = np.dtype([("r", np.ubyte, 1), ("g", np.ubyte, 1), ("b", np.ubyte, 1), ("a", np.ubyte, 1)]) 24. Multiply a 5x3 matrix by a 3x2 matrix (real matrix product) (★☆☆)123456Z = np.dot(np.ones((5,3)), np.ones((3,2)))print(Z)# Alternative solution, in Python 3.5 and aboveZ = np.ones((5,3)) @ np.ones((3,2))print(Z) 25. Given a 1D array, negate all elements which are between 3 and 8, in place. (★☆☆)12345# Author: Evgeni BurovskiZ = np.arange(11)Z[(3 &lt; Z) &amp; (Z &lt;= 8)] *= -1print(Z) 26. What is the output of the following script? (★☆☆)12345# Author: Jake VanderPlasprint(sum(range(5),-1))from numpy import *print(sum(range(5),-1)) 27. Consider an integer vector Z, which of these expressions are legal? (★☆☆)123456Z**Z2 &lt;&lt; Z &gt;&gt; 2Z &lt;- Z1j*ZZ/1/1Z&lt;Z&gt;Z 28. What are the result of the following expressions?123print(np.array(0) / np.array(0))print(np.array(0) // np.array(0))print(np.array([np.nan]).astype(int).astype(float)) 29. How to round away from zero a float array ? (★☆☆)1234# Author: Charles R HarrisZ = np.random.uniform(-10,+10,10)print (np.copysign(np.ceil(np.abs(Z)), Z)) 30. How to find common values between two arrays? (★☆☆)123Z1 = np.random.randint(0,10,10)Z2 = np.random.randint(0,10,10)print(np.intersect1d(Z1,Z2)) 31. How to ignore all numpy warnings (not recommended)? (★☆☆)123456# Suicide mode ondefaults = np.seterr(all="ignore")Z = np.ones(1) / 0# Back to sanity_ = np.seterr(**defaults) An equivalent way, with a context manager: 12with np.errstate(divide='ignore'): Z = np.ones(1) / 0 32. Is the following expressions true? (★☆☆)1np.sqrt(-1) == np.emath.sqrt(-1) 33. How to get the dates of yesterday, today and tomorrow? (★☆☆)123yesterday = np.datetime64('today', 'D') - np.timedelta64(1, 'D')today = np.datetime64('today', 'D')tomorrow = np.datetime64('today', 'D') + np.timedelta64(1, 'D') 34. How to get all the dates corresponding to the month of July 2016? (★★☆)12Z = np.arange('2016-07', '2016-08', dtype='datetime64[D]')print(Z) 35. How to compute ((A+B)*(-A/2)) in place (without copy)? (★★☆)1234567A = np.ones(3)*1B = np.ones(3)*2C = np.ones(3)*3np.add(A,B,out=B)np.divide(A,2,out=A)np.negative(A,out=A)np.multiply(A,B,out=A) 36. Extract the integer part of a random array using 5 different methods (★★☆)1234567Z = np.random.uniform(0,10,10)print (Z - Z%1)print (np.floor(Z))print (np.ceil(Z)-1)print (Z.astype(int))print (np.trunc(Z)) 37. Create a 5x5 matrix with row values ranging from 0 to 4 (★★☆)123Z = np.zeros((5,5))Z += np.arange(5)print(Z) 38. Consider a generator function that generates 10 integers and use it to build an array (★☆☆)12345def generate(): for x in range(10): yield xZ = np.fromiter(generate(),dtype=float,count=-1)print(Z) 39. Create a vector of size 10 with values ranging from 0 to 1, both excluded (★★☆)12Z = np.linspace(0,1,11,endpoint=False)[1:]print(Z) 40. Create a random vector of size 10 and sort it (★★☆)123Z = np.random.random(10)Z.sort()print(Z) 41. How to sum a small array faster than np.sum? (★★☆)1234# Author: Evgeni BurovskiZ = np.arange(10)np.add.reduce(Z) 42. Consider two random array A and B, check if they are equal (★★☆)12345678910A = np.random.randint(0,2,5)B = np.random.randint(0,2,5)# Assuming identical shape of the arrays and a tolerance for the comparison of valuesequal = np.allclose(A,B)print(equal)# Checking both the shape and the element values, no tolerance (values have to be exactly equal)equal = np.array_equal(A,B)print(equal) 43. Make an array immutable (read-only) (★★☆)123Z = np.zeros(10)Z.flags.writeable = FalseZ[0] = 1 44. Consider a random 10x2 matrix representing cartesian coordinates, convert them to polar coordinates (★★☆)123456Z = np.random.random((10,2))X,Y = Z[:,0], Z[:,1]R = np.sqrt(X**2+Y**2)T = np.arctan2(Y,X)print(R)print(T) 45. Create random vector of size 10 and replace the maximum value by 0 (★★☆)123Z = np.random.random(10)Z[Z.argmax()] = 0print(Z) 46. Create a structured array with x and y coordinates covering the [0,1]x[0,1] area (★★☆)1234Z = np.zeros((5,5), [('x',float),('y',float)])Z['x'], Z['y'] = np.meshgrid(np.linspace(0,1,5), np.linspace(0,1,5))print(Z) 47. Given two arrays, X and Y, construct the Cauchy matrix C (Cij =1/(xi - yj))123456# Author: Evgeni BurovskiX = np.arange(8)Y = X + 0.5C = 1.0 / np.subtract.outer(X, Y)print(np.linalg.det(C)) 48. Print the minimum and maximum representable value for each numpy scalar type (★★☆)1234567for dtype in [np.int8, np.int32, np.int64]: print(np.iinfo(dtype).min) print(np.iinfo(dtype).max)for dtype in [np.float32, np.float64]: print(np.finfo(dtype).min) print(np.finfo(dtype).max) print(np.finfo(dtype).eps) 49. How to print all the values of an array? (★★☆)123np.set_printoptions(threshold=np.nan)Z = np.zeros((16,16))print(Z) 50. How to find the closest value (to a given scalar) in a vector? (★★☆)1234Z = np.arange(100)v = np.random.uniform(0,100)index = (np.abs(Z-v)).argmin()print(Z[index]) 51. Create a structured array representing a position (x,y) and a color (r,g,b) (★★☆)123456Z = np.zeros(10, [ ('position', [ ('x', float, 1), ('y', float, 1)]), ('color', [ ('r', float, 1), ('g', float, 1), ('b', float, 1)])])print(Z) 52. Consider a random vector with shape (100,2) representing coordinates, find point by point distances (★★☆)12345678910111213Z = np.random.random((10,2))X,Y = np.atleast_2d(Z[:,0], Z[:,1])D = np.sqrt( (X-X.T)**2 + (Y-Y.T)**2)print(D)# Much faster with scipyimport scipy# Thanks Gavin Heverly-Coulson (#issue 1)import scipy.spatialZ = np.random.random((10,2))D = scipy.spatial.distance.cdist(Z,Z)print(D) 53. How to convert a float (32 bits) array into an integer (32 bits) in place?123Z = np.arange(10, dtype=np.float32)Z = Z.astype(np.int32, copy=False)print(Z) 54. How to read the following file? (★★☆)12345678from io import StringIO# Fake files = StringIO("""1, 2, 3, 4, 5\n 6, , , 7, 8\n , , 9,10,11\n""")Z = np.genfromtxt(s, delimiter=",", dtype=np.int)print(Z) 55. What is the equivalent of enumerate for numpy arrays? (★★☆)12345Z = np.arange(9).reshape(3,3)for index, value in np.ndenumerate(Z): print(index, value)for index in np.ndindex(Z.shape): print(index, Z[index]) 56. Generate a generic 2D Gaussian-like array (★★☆)12345X, Y = np.meshgrid(np.linspace(-1,1,10), np.linspace(-1,1,10))D = np.sqrt(X*X+Y*Y)sigma, mu = 1.0, 0.0G = np.exp(-( (D-mu)**2 / ( 2.0 * sigma**2 ) ) )print(G) 57. How to randomly place p elements in a 2D array? (★★☆)1234567# Author: Divakarn = 10p = 3Z = np.zeros((n,n))np.put(Z, np.random.choice(range(n*n), p, replace=False),1)print(Z) 58. Subtract the mean of each row of a matrix (★★☆)1234567891011# Author: Warren WeckesserX = np.random.rand(5, 10)# Recent versions of numpyY = X - X.mean(axis=1, keepdims=True)# Older versions of numpyY = X - X.mean(axis=1).reshape(-1, 1)print(Y) 59. How to I sort an array by the nth column? (★★☆)12345# Author: Steve TjoaZ = np.random.randint(0,10,(3,3))print(Z)print(Z[Z[:,1].argsort()]) 60. How to tell if a given 2D array has null columns? (★★☆)1234# Author: Warren WeckesserZ = np.random.randint(0,3,(3,10))print((~Z.any(axis=0)).any()) 61. Find the nearest value from a given value in an array (★★☆)1234Z = np.random.uniform(0,1,10)z = 0.5m = Z.flat[np.abs(Z - z).argmin()]print(m) 62. Considering two arrays with shape (1,3) and (3,1), how to compute their sum using an iterator? (★★☆)12345A = np.arange(3).reshape(3,1)B = np.arange(3).reshape(1,3)it = np.nditer([A,B,None])for x,y,z in it: z[...] = x + yprint(it.operands[2]) 63. Create an array class that has a name attribute (★★☆)1234567891011class NamedArray(np.ndarray): def __new__(cls, array, name="no name"): obj = np.asarray(array).view(cls) obj.name = name return obj def __array_finalize__(self, obj): if obj is None: return self.info = getattr(obj, 'name', "no name")Z = NamedArray(np.arange(10), "range_10")print (Z.name) 64. Consider a given vector, how to add 1 to each element indexed by a second vector (be careful with repeated indices)? (★★★)1234567891011# Author: Brett OlsenZ = np.ones(10)I = np.random.randint(0,len(Z),20)Z += np.bincount(I, minlength=len(Z))print(Z)# Another solution# Author: Bartosz Telenczuknp.add.at(Z, I, 1)print(Z) 65. How to accumulate elements of a vector (X) to an array (F) based on an index list (I)? (★★★)123456# Author: Alan G IsaacX = [1,2,3,4,5,6]I = [1,3,9,3,4,1]F = np.bincount(I,X)print(F) 66. Considering a (w,h,3) image of (dtype=ubyte), compute the number of unique colors (★★★)1234567# Author: Nadav Horeshw,h = 16,16I = np.random.randint(0,2,(h,w,3)).astype(np.ubyte)F = I[...,0]*256*256 + I[...,1]*256 +I[...,2]n = len(np.unique(F))print(np.unique(I)) 67. Considering a four dimensions array, how to get sum over the last two axis at once? (★★★)12345678A = np.random.randint(0,10,(3,4,3,4))# solution by passing a tuple of axes (introduced in numpy 1.7.0)sum = A.sum(axis=(-2,-1))print(sum)# solution by flattening the last two dimensions into one# (useful for functions that don't accept tuples for axis argument)sum = A.reshape(A.shape[:-2] + (-1,)).sum(axis=-1)print(sum) 68. Considering a one-dimensional vector D, how to compute means of subsets of D using a vector S of same size describing subset indices? (★★★)123456789101112# Author: Jaime Fernández del RíoD = np.random.uniform(0,1,100)S = np.random.randint(0,10,100)D_sums = np.bincount(S, weights=D)D_counts = np.bincount(S)D_means = D_sums / D_countsprint(D_means)# Pandas solution as a reference due to more intuitive codeimport pandas as pdprint(pd.Series(D).groupby(S).mean()) 69. How to get the diagonal of a dot product? (★★★)12345678910111213# Author: Mathieu BlondelA = np.random.uniform(0,1,(5,5))B = np.random.uniform(0,1,(5,5))# Slow versionnp.diag(np.dot(A, B))# Fast versionnp.sum(A * B.T, axis=1)# Faster versionnp.einsum("ij,ji-&gt;i", A, B) 70. Consider the vector [1, 2, 3, 4, 5], how to build a new vector with 3 consecutive zeros interleaved between each value? (★★★)1234567# Author: Warren WeckesserZ = np.array([1,2,3,4,5])nz = 3Z0 = np.zeros(len(Z) + (len(Z)-1)*(nz))Z0[::nz+1] = Zprint(Z0) 71. Consider an array of dimension (5,5,3), how to mulitply it by an array with dimensions (5,5)? (★★★)123A = np.ones((5,5,3))B = 2*np.ones((5,5))print(A * B[:,:,None]) 72. How to swap two rows of an array? (★★★)12345# Author: Eelco HoogendoornA = np.arange(25).reshape(5,5)A[[0,1]] = A[[1,0]]print(A) 73. Consider a set of 10 triplets describing 10 triangles (with shared vertices), find the set of unique line segments composing all the triangles (★★★)123456789# Author: Nicolas P. Rougierfaces = np.random.randint(0,100,(10,3))F = np.roll(faces.repeat(2,axis=1),-1,axis=1)F = F.reshape(len(F)*3,2)F = np.sort(F,axis=1)G = F.view( dtype=[('p0',F.dtype),('p1',F.dtype)] )G = np.unique(G)print(G) 74. Given an array C that is a bincount, how to produce an array A such that np.bincount(A) == C? (★★★)12345# Author: Jaime Fernández del RíoC = np.bincount([1,1,2,3,4,4,6])A = np.repeat(np.arange(len(C)), C)print(A) 75. How to compute averages using a sliding window over an array? (★★★)12345678# Author: Jaime Fernández del Ríodef moving_average(a, n=3) : ret = np.cumsum(a, dtype=float) ret[n:] = ret[n:] - ret[:-n] return ret[n - 1:] / nZ = np.arange(20)print(moving_average(Z, n=3)) 76. Consider a one-dimensional array Z, build a two-dimensional array whose first row is (Z[0],Z[1],Z[2]) and each subsequent row is shifted by 1 (last row should be (Z[-3],Z[-2],Z[-1]) (★★★)123456789# Author: Joe Kington / Erik Rigtorpfrom numpy.lib import stride_tricksdef rolling(a, window): shape = (a.size - window + 1, window) strides = (a.itemsize, a.itemsize) return stride_tricks.as_strided(a, shape=shape, strides=strides)Z = rolling(np.arange(10), 3)print(Z) 77. How to negate a boolean, or to change the sign of a float inplace? (★★★)1234567# Author: Nathaniel J. SmithZ = np.random.randint(0,2,100)np.logical_not(Z, out=Z)Z = np.random.uniform(-1.0,1.0,100)np.negative(Z, out=Z) 78. Consider 2 sets of points P0,P1 describing lines (2d) and a point p, how to compute distance from p to each line i (P0[i],P1[i])? (★★★)123456789101112def distance(P0, P1, p): T = P1 - P0 L = (T**2).sum(axis=1) U = -((P0[:,0]-p[...,0])*T[:,0] + (P0[:,1]-p[...,1])*T[:,1]) / L U = U.reshape(len(U),1) D = P0 + U*T - p return np.sqrt((D**2).sum(axis=1))P0 = np.random.uniform(-10,10,(10,2))P1 = np.random.uniform(-10,10,(10,2))p = np.random.uniform(-10,10,( 1,2))print(distance(P0, P1, p)) 79. Consider 2 sets of points P0,P1 describing lines (2d) and a set of points P, how to compute distance from each point j (P[j]) to each line i (P0[i],P1[i])? (★★★)1234567# Author: Italmassov Kuanysh# based on distance function from previous questionP0 = np.random.uniform(-10, 10, (10,2))P1 = np.random.uniform(-10,10,(10,2))p = np.random.uniform(-10, 10, (10,2))print(np.array([distance(P0,P1,p_i) for p_i in p])) 80. Consider an arbitrary array, write a function that extract a subpart with a fixed shape and centered on a given element (pad with a fill value when necessary) (★★★)123456789101112131415161718192021222324252627# Author: Nicolas RougierZ = np.random.randint(0,10,(10,10))shape = (5,5)fill = 0position = (1,1)R = np.ones(shape, dtype=Z.dtype)*fillP = np.array(list(position)).astype(int)Rs = np.array(list(R.shape)).astype(int)Zs = np.array(list(Z.shape)).astype(int)R_start = np.zeros((len(shape),)).astype(int)R_stop = np.array(list(shape)).astype(int)Z_start = (P-Rs//2)Z_stop = (P+Rs//2)+Rs%2R_start = (R_start - np.minimum(Z_start,0)).tolist()Z_start = (np.maximum(Z_start,0)).tolist()R_stop = np.maximum(R_start, (R_stop - np.maximum(Z_stop-Zs,0))).tolist()Z_stop = (np.minimum(Z_stop,Zs)).tolist()r = [slice(start,stop) for start,stop in zip(R_start,R_stop)]z = [slice(start,stop) for start,stop in zip(Z_start,Z_stop)]R[r] = Z[z]print(Z)print(R) 81. Consider an array Z = [1,2,3,4,5,6,7,8,9,10,11,12,13,14], how to generate an array R = [[1,2,3,4], [2,3,4,5], [3,4,5,6], …, [11,12,13,14]]? (★★★)12345# Author: Stefan van der WaltZ = np.arange(1,15,dtype=np.uint32)R = stride_tricks.as_strided(Z,(11,4),(4,4))print(R) 82. Compute a matrix rank (★★★)123456# Author: Stefan van der WaltZ = np.random.uniform(0,1,(10,10))U, S, V = np.linalg.svd(Z) # Singular Value Decompositionrank = np.sum(S &gt; 1e-10)print(rank) 83. How to find the most frequent value in an array?12Z = np.random.randint(0,10,50)print(np.bincount(Z).argmax()) 84. Extract all the contiguous 3x3 blocks from a random 10x10 matrix (★★★)12345678# Author: Chris BarkerZ = np.random.randint(0,5,(10,10))n = 3i = 1 + (Z.shape[0]-3)j = 1 + (Z.shape[1]-3)C = stride_tricks.as_strided(Z, shape=(i, j, n, n), strides=Z.strides + Z.strides)print(C) 85. Create a 2D array subclass such that Z[i,j] == Z[j,i] (★★★)123456789101112131415# Author: Eric O. Lebigot# Note: only works for 2d array and value setting using indicesclass Symetric(np.ndarray): def __setitem__(self, index, value): i,j = index super(Symetric, self).__setitem__((i,j), value) super(Symetric, self).__setitem__((j,i), value)def symetric(Z): return np.asarray(Z + Z.T - np.diag(Z.diagonal())).view(Symetric)S = symetric(np.random.randint(0,10,(5,5)))S[2,3] = 42print(S) 86. Consider a set of p matrices wich shape (n,n) and a set of p vectors with shape (n,1). How to compute the sum of of the p matrix products at once? (result has shape (n,1)) (★★★)12345678910111213# Author: Stefan van der Waltp, n = 10, 20M = np.ones((p,n,n))V = np.ones((p,n,1))S = np.tensordot(M, V, axes=[[0, 2], [0, 1]])print(S)# It works, because:# M is (p,n,n)# V is (p,n,1)# Thus, summing over the paired axes 0 and 0 (of M and V independently),# and 2 and 1, to remain with a (n,1) vector. 87. Consider a 16x16 array, how to get the block-sum (block size is 4x4)? (★★★)1234567# Author: Robert KernZ = np.ones((16,16))k = 4S = np.add.reduceat(np.add.reduceat(Z, np.arange(0, Z.shape[0], k), axis=0), np.arange(0, Z.shape[1], k), axis=1)print(S) 88. How to implement the Game of Life using numpy arrays? (★★★)123456789101112131415161718# Author: Nicolas Rougierdef iterate(Z): # Count neighbours N = (Z[0:-2,0:-2] + Z[0:-2,1:-1] + Z[0:-2,2:] + Z[1:-1,0:-2] + Z[1:-1,2:] + Z[2: ,0:-2] + Z[2: ,1:-1] + Z[2: ,2:]) # Apply rules birth = (N==3) &amp; (Z[1:-1,1:-1]==0) survive = ((N==2) | (N==3)) &amp; (Z[1:-1,1:-1]==1) Z[...] = 0 Z[1:-1,1:-1][birth | survive] = 1 return ZZ = np.random.randint(0,2,(50,50))for i in range(100): Z = iterate(Z)print(Z) 89. How to get the n largest values of an array (★★★)123456789Z = np.arange(10000)np.random.shuffle(Z)n = 5# Slowprint (Z[np.argsort(Z)[-n:]])# Fastprint (Z[np.argpartition(-Z,n)[:n]]) 90. Given an arbitrary number of vectors, build the cartesian product (every combinations of every item) (★★★)123456789101112131415# Author: Stefan Van der Waltdef cartesian(arrays): arrays = [np.asarray(a) for a in arrays] shape = (len(x) for x in arrays) ix = np.indices(shape, dtype=int) ix = ix.reshape(len(arrays), -1).T for n, arr in enumerate(arrays): ix[:, n] = arrays[n][ix[:, n]] return ixprint (cartesian(([1, 2, 3], [4, 5], [6, 7]))) 91. How to create a record array from a regular array? (★★★)123456Z = np.array([("Hello", 2.5, 3), ("World", 3.6, 2)])R = np.core.records.fromarrays(Z.T, names='col1, col2, col3', formats = 'S8, f8, i8')print(R) 92. Consider a large vector Z, compute Z to the power of 3 using 3 different methods (★★★)1234567# Author: Ryan G.x = np.random.rand(5e7)%timeit np.power(x,3)%timeit x*x*x%timeit np.einsum('i,i,i-&gt;i',x,x,x) 93. Consider two arrays A and B of shape (8,3) and (2,2). How to find rows of A that contain elements of each row of B regardless of the order of the elements in B? (★★★)12345678# Author: Gabe SchwartzA = np.random.randint(0,5,(8,3))B = np.random.randint(0,5,(2,2))C = (A[..., np.newaxis, np.newaxis] == B)rows = np.where(C.any((3,1)).all(1))[0]print(rows) 94. Considering a 10x3 matrix, extract rows with unequal values (e.g. [2,2,3]) (★★★)1234567891011# Author: Robert KernZ = np.random.randint(0,5,(10,3))print(Z)# solution for arrays of all dtypes (including string arrays and record arrays)E = np.all(Z[:,1:] == Z[:,:-1], axis=1)U = Z[~E]print(U)# soluiton for numerical arrays only, will work for any number of columns in ZU = Z[Z.max(axis=1) != Z.min(axis=1),:]print(U) 95. Convert a vector of ints into a matrix binary representation (★★★)12345678910# Author: Warren WeckesserI = np.array([0, 1, 2, 3, 15, 16, 32, 64, 128])B = ((I.reshape(-1,1) &amp; (2**np.arange(8))) != 0).astype(int)print(B[:,::-1])# Author: Daniel T. McDonaldI = np.array([0, 1, 2, 3, 15, 16, 32, 64, 128], dtype=np.uint8)print(np.unpackbits(I[:, np.newaxis], axis=1)) 96. Given a two dimensional array, how to extract unique rows? (★★★)1234567# Author: Jaime Fernández del RíoZ = np.random.randint(0,2,(6,3))T = np.ascontiguousarray(Z).view(np.dtype((np.void, Z.dtype.itemsize * Z.shape[1])))_, idx = np.unique(T, return_index=True)uZ = Z[idx]print(uZ) 97. Considering 2 vectors A &amp; B, write the einsum equivalent of inner, outer, sum, and mul function (★★★)12345678910# Author: Alex Riley# Make sure to read: http://ajcr.net/Basic-guide-to-einsum/A = np.random.uniform(0,1,10)B = np.random.uniform(0,1,10)np.einsum('i-&gt;', A) # np.sum(A)np.einsum('i,i-&gt;i', A, B) # A * Bnp.einsum('i,i', A, B) # np.inner(A, B)np.einsum('i,j-&gt;ij', A, B) # np.outer(A, B) 98. Considering a path described by two vectors (X,Y), how to sample it using equidistant samples (★★★)?12345678910111213# Author: Bas Swinckelsphi = np.arange(0, 10*np.pi, 0.1)a = 1x = a*phi*np.cos(phi)y = a*phi*np.sin(phi)dr = (np.diff(x)**2 + np.diff(y)**2)**.5 # segment lengthsr = np.zeros_like(x)r[1:] = np.cumsum(dr) # integrate pathr_int = np.linspace(0, r.max(), 200) # regular spaced pathx_int = np.interp(r_int, r, x) # integrate pathy_int = np.interp(r_int, r, y) 99. Given an integer n and a 2D array X, select from X the rows which can be interpreted as draws from a multinomial distribution with n degrees, i.e., the rows which only contain integers and which sum to n. (★★★)123456789# Author: Evgeni BurovskiX = np.asarray([[1.0, 0.0, 3.0, 8.0], [2.0, 0.0, 1.0, 1.0], [1.5, 2.5, 1.0, 0.0]])n = 4M = np.logical_and.reduce(np.mod(X, 1) == 0, axis=-1)M &amp;= (X.sum(axis=-1) == n)print(X[M]) 100. Compute bootstrapped 95% confidence intervals for the mean of a 1D array X (i.e., resample the elements of an array with replacement N times, compute the mean of each sample, and then compute percentiles over the means). (★★★)12345678# Author: Jessica B. HamrickX = np.random.randn(100) # random 1D arrayN = 1000 # number of bootstrap samplesidx = np.random.randint(0, X.size, (N, X.size))means = X[idx].mean(axis=1)confint = np.percentile(means, [2.5, 97.5])print(confint)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反向传播（二）]]></title>
    <url>%2F2018%2F07%2F26%2Fback-propagation-2%2F</url>
    <content type="text"><![CDATA[如何让多层神经网络学习呢？我们已了解了使用梯度下降来更新权重，反向传播算法则是它的一个延伸。要使用梯度下降法更新隐藏层的权重，你需要知道各隐藏层节点的误差对最终输出的影响。每层的输出是由两层间的权重决定的，两层之间产生的误差，按权重缩放后在网络中向前传播。既然我们知道输出误差，便可以用权重来反向传播到隐藏层。我们已经知道一个权重的更新可以这样计算： \Delta w_{i} = \eta\delta_j x_i这里 $\delta$(error term) 是指： \delta_j = (y_j-\hat{y_j})f^\prime(h_j) = (y_j-\hat{y_j})f^\prime_j(\sum_{}w_ix_i)上面公式中 $(y_j-\hat{y_j})$ 是输出误差，激活函数 $f(h_j)$ 的导函是 $f^\prime(h_j)$ ，我们把这个导函数称做输出的梯度。 网上反向传播的版本很多，看千百个版本不如自己手推一个版本。 定义 层 - 用带括号的字母L表示层，(L)表示第L层，(L-1)表示第L-1层，也就是L层的上一层，(L+1)表示第L+1层，也就是L层的下一层。 下标 - 用 i ，j ， k ，分别表示第 L-1 层的任意节点，第 L 层的任意节点，以及第 L+1 层的任意节点。 $a^{(L)}_i$ - 表示节点 i 在第 L 层的输出。 $w^{(L)}_{ij}$ - 表示在节点 i 和节点 j之间的权重，它代表第 L 层的权重。 $o^{(L)}_i$ - 表示节点 i 在第 L 层的输出。 $r^{(L)}$ - 表示第 L 层的节点数量 $b^{(L)}_{i}$ - 表示节点 i 在第 L 层的偏差 $\sigma()$ - 表示激活函数 sigmoid 前置条件本文的的激活函数不再用 f(x) 表示，而用 sigmoid 函数作为激活函数，表示为 $\sigma()$。 为了进一步简化，对于节点 j 在第 L 层 的偏差 $b^{(L)}_{j}$ 将被合并到权重中，表示为 $w^{(L)}_{0j}$： w^{(L)}_{0j} = b^{(L)}_j那么节点 j 的输出： a^{(L)}_j = b^{(L)}_j + \sum^{r^{(L-1)}}_{i=1} w^{(L)}_{ij}o^{(L-1)}_i = \sum^{r^{(L-1)}}_{i=0} w^{(L)}_{ij}o^{(L-1)}_i误差函数我们依然用 SSE 作为衡量预测结果的标准： E = \frac{1}{2}(y - \hat{y})^2误差偏导反向传播算法的推导通过将链式法则应用于误差函数偏导开始： \frac{\alpha E}{\alpha w^{(L)}_{ij}} = \frac{\alpha E}{\alpha a^{(L)}_{j}}\frac{\alpha a^{(L)}_j}{\alpha w^{(L)}_{ij}}误差项依然用 $\delta$ 表示，不同的是带了上下标，表示节点 j 在第 L 层的误差项： \delta^{(L)}_j =\frac{\alpha E}{\alpha a^{(L)}_{j}}而 a^{(L)}_j = \sum^{r^{(L-1)}}_{i=0}w^{(L)}_{ij}o^{(L-1)}_i$o^{(L-1)}_i$ 表示节点 i 在第 L-1 层的输出，求偏导可知： \frac{\alpha a^{(L)}_j}{\alpha w^{(L)}_{ij}} = o^{(L-1)}_i因此 \frac{\alpha E}{\alpha w^{(L)}_{ij}} = \delta^{(L)}_jo^{(L-1)}_i输出层这里我们假设输出层只有一个节点，因此节点的下标是1而不是 k ，那么最后一层的输出表示为$a^{(L+1)}_1$，由误差函数可知： E = \frac{1}{2}(y - \hat{y})^2 = \frac{1}{2}(y-\sigma(a^{(L+1)}_1))^2$\sigma()$ 是激活函数 sigmoid 函数。同样令输出层的误差项为 $\delta^{(L+1)}_1 = \frac{\alpha E}{\alpha a^{(L+1)}_{1}}$，所以 \delta^{(L+1)}_1 = \frac{\alpha E}{\alpha a^{(L+1)}_{1}} = -(y-\sigma(a^{(L+1)}_1))\sigma^\prime(a^{(L+1)}_1) = -(y-\hat{y})\sigma^\prime(a^{(L+1)}_1)隐藏层对于隐藏层节点 j 的误差项： \delta^{(L)}_j = \frac{\alpha E}{\alpha a^{(L)}_{j}} = \sum^{r^{(L+1)}}_{k=1}\frac{\alpha E}{\alpha a^{(L+1)}_{k}}\frac{\alpha a^{(L+1)}_k}{\alpha a^{(L)}_j}上式公式中为什么要求和？请看下图：因为激活值可以通过不同的途径影响代价函数(误差函数)，也就是说神经元一边通过 $a^{(L+1)}_1$ 来影响代价函数，另一边通过 $a^{(L+1)}_2$ 来影响代价函数，得把这些都加起来。至于求和公式中 k 是从1开始的，因为输出层没有偏差，只有输出节点，而从0开始表示该层包含一个偏差的节点。 令 $\delta^{(L+1)}_k = \frac{\alpha E}{\alpha a^{(L+1)}_{k}}$ ，所以 \delta^{(L)}_j = \sum^{r^{(L+1)}}_{k=1}\delta^{(L+1)}_k\frac{\alpha a^{(L+1)}_k}{\alpha a^{(L)}_j}由于 a^{(L+1)}_k = \sum^{r^{(L)}}_{j=0}w^{(L+1)}_{jk}\sigma(a^{(L)}_j)所以 \frac{\alpha a^{(L+1)}_k}{\alpha a^{(L)}_j} = w^{(L+1)}_{jk}\sigma^\prime(a^{(L)}_j)因此 \delta^{(L)}_j = \sum^{r^{(L+1)}}_{k=1}\delta^{(L+1)}_kw^{(L+1)}_{jk}\sigma^\prime(a^{(L)}_j) = \sigma^\prime(a^{(L)}_j)\sum^{r^{(L+1)}}_{k=1}w^{(L+1)}_{jk}\delta^{(L+1)}_k最后带入公式 \frac{\alpha E}{\alpha w^{(L)}_{ij}} = \delta^{(L)}_jo^{(L-1)}_i = \sigma^\prime(a^{(L)}_j)o^{(L-1)}_i\sum^{r^{(L+1)}}_{k=1}w^{(L+1)}_{jk}\delta^{(L+1)}_k至此，反向传播推导就结束了。 总结上面推导出了很多公式，总结一下对我们有用的公式： 误差函数对权重的偏导，可以理解为代价函数（误差）对权重 w 的微小变化有多敏感\frac{\alpha E}{\alpha w^{(L)}_{ij}} = \delta^{(L)}_jo^{(L-1)}_i 输出层的误差项\delta^{(L+1)}_1 = -(y-\hat{y})\sigma^\prime(a^{(L+1)}_1) 隐藏层误差项\delta^{(L)}_j = \sigma^\prime(a^{(L)}_j)\sum^{r^{(L+1)}}_{k=1}w^{(L+1)}_{jk}\delta^{(L+1)}_k 例子1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import numpy as npfrom data_prep import features, targets, features_test, targets_testnp.random.seed(21)def sigmoid(x): &quot;&quot;&quot; Calculate sigmoid &quot;&quot;&quot; return 1 / (1 + np.exp(-x))# 超参数n_hidden = 2 # number of hidden unitsepochs = 900learnrate = 0.005n_records, n_features = features.shapelast_loss = None# 初始化权重weights_input_hidden = np.random.normal(scale=1 / n_features ** .5, size=(n_features, n_hidden))weights_hidden_output = np.random.normal(scale=1 / n_features ** .5, size=n_hidden)print(&apos;weights_input_hidden=&#123;&#125;&apos;.format(weights_input_hidden))print(&apos;weights_hidden_output=&#123;&#125;&apos;.format(weights_hidden_output))for e in range(epochs): del_w_input_hidden = np.zeros(weights_input_hidden.shape) del_w_hidden_output = np.zeros(weights_hidden_output.shape) for x, y in zip(features.values, targets): ## 前向传播 ## # TODO: 计算输出 #print(&apos;x=&#123;&#125;&apos;.format(x)) hidden_input = np.dot(x, weights_input_hidden) hidden_output = sigmoid(hidden_input) output = sigmoid(np.dot(hidden_output, weights_hidden_output)) ## 后向传播 ## # TODO: 实际值与期望的差值 error = y - output # TODO: 对输出求误差梯度 output_error_term = error * output * (1-output) ## 传播误差到隐藏层 # TODO: 计算隐藏层对误差的贡献 hidden_error = np.dot(weights_hidden_output, output_error_term) # TODO: 对隐藏层求误差梯度 hidden_error_term = hidden_error * hidden_output * (1-hidden_output) # TODO: 更新权重的变化率 del_w_hidden_output += output_error_term * hidden_output del_w_input_hidden += hidden_error_term * x[:, None] # TODO: 更新权重 weights_input_hidden += learnrate * del_w_input_hidden / n_records weights_hidden_output += learnrate * del_w_hidden_output / n_records # 打印训练集上的均方差 if e % (epochs / 10) == 0: hidden_output = sigmoid(np.dot(x, weights_input_hidden)) out = sigmoid(np.dot(hidden_output, weights_hidden_output)) loss = np.mean((out - targets) ** 2) if last_loss and last_loss &lt; loss: print(&quot;Train loss: &quot;, loss, &quot; WARNING - Loss Increasing&quot;) else: print(&quot;Train loss: &quot;, loss) last_loss = loss# 对测试数据计算准确度hidden = sigmoid(np.dot(features_test, weights_input_hidden))out = sigmoid(np.dot(hidden, weights_hidden_output))predictions = out &gt; 0.5accuracy = np.mean(predictions == targets_test)print(&quot;Prediction accuracy: &#123;:.3f&#125;&quot;.format(accuracy)) 参考文献 Brilliant-Backpropagation 反向传播的微积分原理]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反向传播（一）]]></title>
    <url>%2F2018%2F07%2F20%2Fback-propagation-1%2F</url>
    <content type="text"><![CDATA[反向传播是神经网络的基石，它的名字也很容易理解，因为梯度的计算是通过网络向后进行，首先计算最终权重层的梯度，并且最后计算第一层权重的梯度。在计算前一层梯度时，重复使用来自上一层梯度的计算部分。这种误差信息的向后流动就形象的称为反向传播。 链式法则首先介绍下链式法则，他是反向传播的核心，链式法则也即复合函数求导的过程。链式法则的内容是：如果有一个变量 x ，以及一个关于 x 的函数 f(x) 我们将简称为 A ，另一个函数 g ，将 f(x) 作为 g 的变量得到 g o f(x) 。链式法则证明： B 关于 x 的偏导数，就是 B 关于 A 的偏导数，乘以 A 关于 x 的偏导数，所以对复合函数求导就是一系列导数的乘积。 SSE怎么样衡量预测结果的标准？最简单，最容易想到的是用实际目标值 y 减去网络输出值 ŷ ，以两者差值衡量误差。 E = y - \hat{y}然而，若预测值高于目标值，那么差值就为负数，若预测值低于目标值，差值为正数，我们希望误差能够保持符号一致，要让符号全部归正，可以求差值的平方： E = (y - \hat{y})^2你可能在想，为什么不直接用绝对值呢？问得好！这是因为使用平方值时，异常数值会被赋予更高的惩罚值，而较小误差的惩罚值则较低。 目前我们仅得到单次预测的误差，我们希望求出全体数据的整体误差，那么可以对每项数据 μ 的误差求和: E = \sum_{μ}(y - \hat{y})^2最后我们在式子前面加上$\frac{1}{2}$，以便简化后续计算。怎么个简化法？这是为了后面求导时可以跟平方数抵消，那么最后的公式： E = \frac{1}{2}\sum_{μ}(y - \hat{y})^2此公式通常称为误差平方和，简称 SSE(sum of the squared errors)。误差平方和，可用于衡量神经网络的预测效果，值越高，预测效果越差，值越低，预测效果越好，这也是刚才我们求完差值后为什么用平方而不是取绝对值的原因。 梯度下降这里举一个简化的例子来阐述梯度下降的过程，我们只考虑单行数据的情况。上面SSE的公式中全体数据用 μ 表示，我们可以将这些数据看成两组表格、数组或者矩阵,其中一组包含输入值，另一组包含目标值 y ，每项数据占一行位置 μ = 1 表示第一行数据，如果需要计算整体误差，可以逐行计算误差平方和，然后对所得结果求和。我们将数据带入到公式中： E = \frac{1}{2}(y - \hat{y})^2 = \frac{1}{2}(y - f(\sum_{i}w_ix_i))^2展开预测值之后，可以看出权重是误差函数的参数，我们的目标是求取能使误差最小化的权重值，如下图：由上图知，我们的目标就是求取图形碗底对应的权值，从某个随机权值出发，逐步向误差最小值的方向前进，这个方向与梯度（斜率）相反，只要始终沿着梯度反复逐步下降，最终能求得对应最小误差的权值，这就是梯度下降的过程。下面来更新权值，新的权值 $w_i$ 等于旧的权值 $w_i$ 加上更新步长$\Delta w_i$： w_i = w_i + \Delta w_i由于前述更新步长与梯度成正比，而梯度等于误差关于每个权重$w_i$的偏导数： \Delta w_i \propto -\frac{\alpha E}{\alpha w_i}公式中还需添加一个缩放系数变量用来控制梯度下降中更新步长的大小： \Delta w_i = -\eta\frac{\alpha E}{\alpha w_i}这个变量叫作学习速率用希腊字母 $\eta$ 表示。对于计算梯度需要用到多元微积分，也就是前面提到的链式法则。下面我们展开计算梯度： \frac{\alpha E}{\alpha w_i} = \frac{\alpha}{\alpha w_i}\frac{1}{2}(y-\hat{y})^2 = \frac{\alpha}{\alpha w_i}\frac{1}{2}(y-\hat{y}(w_i))^2鉴于输出值 $\hat{y}$ 是权重的函数，这里相当于复合函数，可以通过链式法则来求偏导，前面已经介绍过了，那么： \frac{\alpha E}{\alpha w_i} = (y-\hat{y})\frac{\alpha}{\alpha w_i}(y-\hat{y}) = -(y-\hat{y})\frac{\alpha \hat{y}}{\alpha w_i}下面再来对 $\hat{y}$ 求偏导，我们知道 \hat{y} =f(h)而 h = \sum_{i}w_ix_i因此 \frac{\alpha E}{\alpha w_i} = -(y-\hat{y})f^\prime(h)\frac{\alpha}{\alpha w_i}\sum_{i}w_ix_i我们把上式的最后一项单独提出来，在求和式子中，每个权重仅是单个子项的参数： \frac{\alpha}{\alpha w_i}\sum_{i}w_ix_i将其展开 \frac{\alpha}{\alpha w_i}\sum_{i}w_ix_i = \frac{\alpha}{\alpha w_i}[w_1x_1+w_2x_2+...+w_nx_n] = x_i+0+0+0+...综合来看，误差平方关于 $w_i$ 的偏导数等于\frac{\alpha E}{\alpha w_i} = -(y-\hat{y})f^\prime(h)x_i所以 \Delta w_i = \eta(y-\hat{y})f^\prime(h)x_i最后我们得出结论，更新步长就等于学习速率 $\eta$ 乘以预测差值，乘以激活函数导数，再乘以输入值，为方便后续应用，我们将预测差值乘以激活函数的导数用小写字母 $\delta$ 表示 \delta = (y-\hat{y})f^\prime(h)那么权值更新公式可以重新写为w_i = w_i+\eta\delta x_i以上便是单行数据的情况，你的神经网络中可能会有多个输出单元，可以将其视为单个输出网络的堆叠架构，但需将输入单元连接到新的输出单元，这时整体误差等于每个输出单元的误差之和。梯度下降法可以扩展适用于这种情况，只需分别计算每个输出单元的误差项\delta_j = (y_j-\hat{y}_j)f^\prime(h_j)\Delta w_{ij} = \eta\delta_jx_i以上便是反向传播的核心知识点，下一章，对反向传播进行数学推导。]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前向传播]]></title>
    <url>%2F2018%2F07%2F20%2Fforward-propagation%2F</url>
    <content type="text"><![CDATA[前向传播是神经网络用来将输入变成输出的流程。 输入层-&gt;隐藏层在下面这个简单的网络图中，输入单元被标注为 $x_{_1}$​，$x_{_2}$​，$x_{_3}$​，隐藏层节点是 $h_{_1}$​，$h_{_2}$。这个网络图中有多个输入单元和多个隐藏单元，它们的权重需要有两个索引 $w_{_{ij}}$​，其中 i 表示输入单元，j 表示隐藏单元。 为了定位权重，我们把输入节点的索引 i 和隐藏节点的索引 j​ 结合，得到：$w_{_{11}}$ - 代表从 $x_{_1}$​ 到 $h_{_1}$​ 的权重；$w_{_{12}}$ - 代表从 $x_{_1}$ 到 $h_{_2}$ 的权重。… 下图包括了从输入层到隐藏层的所有权重，用 $w_{_{ij}}$​ 表示：现在，将权重储存在矩阵中，由 $w_{_{ij}}$​ 来索引。矩阵中的每一行对应从同一个输入节点发出的权重，每一列对应传入同一个隐藏节点的权重。这里我们有三个输入节点，两个隐藏节点，权重矩阵表示为：运用矩阵乘法可以得出每一个隐藏层节点 $h_{_j}$： h_{_j} = \sum_{i}x_{_i}w_{_{ij}}那么： h_{_1} = x_{_1}w_{_{11}} + x_{_2}w_{_{21}} + x_{_3}w_{_{31}}以此类推，可以求出第二个隐藏节点$h_{_2}$： h_{_2} = x_{_1}w_{_{12}} + x_{_2}w_{_{22}} + x_{_3}w_{_{32}}在 NumPy 中，我们可以直接使用 np.dot：hidden_inputs = np.dot(inputs, weights_input_to_hidden) 上面的网络图是不含偏差的，实际上加上偏差的网络图前向传播是类似的:如上图具有上标(1)的权重属于第一层，具有上标(2)的权重属于第二层级，偏差不再叫b，现在叫做 $W_{31}$， $W_{_{32}}$ 等等。 在第一层级中，我们执行前向传播可以得出： h_1 = W^{^{(1)}}_{_{11}}x_{_1} + W^{^{(1)}}_{_{21}}x_{_2} + W^{^{(1)}}_{_{31}}h_2 = W^{^{(1)}}_{_{12}}x_{_1} + W^{^{(1)}}_{_{22}}x_{_2} + W^{^{(1)}}_{_{32}}我们再给每层加上一个激活函数，选定 sigmoid 函数为我们的激活函数。先简单介绍下 sigmoid 函数。 sigmoid 函数sigmoid 函数是一个有着优美S形曲线的数学函数，在逻辑回归、人工神经网络中有着广泛的应用。它的数学形式： \sigma(x) = \frac{1}{1+e^{-x}}其函数图像如下：可以看出，sigmoid 函数连续，光滑，严格单调，以(0,0.5)中心对称，是一个非常良好的阈值函数。sigmoid 函数的值域范围限制在(0,1)之间，我们知道[0,1]与概率值的范围是相对应的，这样sigmoid函数就能与一个概率分布联系起来了。 sigmoid 函数的导数sigmoid 函数有一个美丽的导数，我们可以在下面的计算中看到。这将使我们的反向传播步骤更加简洁，这也是这里为什么选用它为激活函数的原因。 \sigma^{'}(x) = \frac{\alpha}{\alpha x}\frac{1}{1+e^{-x}} = \frac{e^{-x}}{(1+e^{-x})^2} = \frac{1}{1+e^{-x}}\frac{e^{-x}}{1+e^{-x}} = \sigma(x)(1-\sigma(x))隐藏层节点通过 sigmoid 激活函数可以分别求得隐藏层的输出为:hide_out_1 = $\sigma(h_1)$hide_out_2 = $\sigma(h_2)$ 隐藏层-&gt;输出层通过线性方程，结合隐藏层的输出可以求出输出节点： h = W^{^{(2)}}_{_{11}}\sigma(h_1) + W^{^{(2)}}_{_{21}}\sigma(h_2) + W^{^{(2)}}_{_{31}}对输出节点应用 sigmoid 函数，得出预测： \hat{y} = \sigma(h)即在 0 到 1 之间的概率 用 ŷ 表示。至此，前向传播就结束了。]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n课程中文笔记汇总]]></title>
    <url>%2F2018%2F07%2F18%2FCS231n-notes%2F</url>
    <content type="text"><![CDATA[CS231n简介CS231n的全称是CS231n: Convolutional Neural Networks for Visual Recognition，即面向视觉识别的卷积神经网络。该课程是斯坦福大学计算机视觉实验室推出的课程。需要注意的是，目前大家说CS231n，大都指的是2016年冬季学期（一月到三月）的最新版本。 对于不喜欢看英文笔记的同学，本文收集了CS231n课程的中文笔记。笔记来源：智能单元 - 知乎专栏。 索引CS231n官方笔记授权翻译总集篇 Python Numpy教程 图像分类笔记（上） 图像分类笔记（下） 线性分类笔记（上） 线性分类笔记（中） 线性分类笔记（下） 最优化笔记（上） 最优化笔记（下） 反向传播笔记 神经网络笔记1（上） 神经网络笔记1（下） 神经网络笔记 2 神经网络笔记3（上） 神经网络笔记3（下） 卷积神经网络笔记 课程作业# 1简介 课程作业# 2简介 课程作业# 3简介]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络中的维度]]></title>
    <url>%2F2018%2F07%2F12%2Fcnn-dimension%2F</url>
    <content type="text"><![CDATA[神经网络的维度是个经常让人迷糊的概念，本文以卷积神经网络为例介绍怎么计算维度。文中的公式也可以作为知识点backup。 维度先来个例子，按以下步骤用Keras创建一个简单的CNN。 首先创建一个序列模型，使用add() 方法向网络中添加层级：1234567from keras.models import Sequentialfrom keras.layers import Conv2Dmodel = Sequential()model.add(Conv2D(filters=16, kernel_size=2, strides=2, padding=&apos;valid&apos;, activation=&apos;relu&apos;, input_shape=(200, 200, 1)))model.summary() 让我们通过该网络所提供的参数研究卷积层的维度如何变化，你可以在jupyter notebook上运行。现在来看看输出结果是什么： 结果显示该卷积层有80个参数，对应的输出在Param #。同时注意卷积层的形状是如何变化，对应输出内容中的Output Shape,上图中，None对应的是批次大小，卷积层的高度为100，宽度为100，深度为16。 本文以Keras来搭建卷积网络，让我们先来了解下卷积层（Conv2D）的参数。 Conv2D123456789101112131415keras.layers.convolutional.Conv2D(filters, kernel_size, strides=(1, 1), padding=&apos;valid&apos;, data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer=&apos;glorot_uniform&apos;, bias_initializer=&apos;zeros&apos;, kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) 二维卷积层，即对图像的空域卷积。该层对二维输入进行滑动窗卷积，当使用该层作为第一层时，应提供input_shape参数。例如input_shape = (128,128,3)代表128*128的彩色RGB图像（data_format=’channels_last’） 参数 filters：卷积核的数目（即输出的维度） kernel_size：单个整数或由两个整数构成的list/tuple，卷积核的宽度和长度。如为单个整数，则表示在各个空间维度的相同长度。 strides：单个整数或由两个整数构成的list/tuple，为卷积的步长。如为单个整数，则表示在各个空间维度的相同步长。任何不为1的strides均与任何不为1的dilation_rate均不兼容 padding：补0策略，为“valid”, “same” 。“valid”代表只进行有效的卷积，即对边界数据不处理。“same”代表保留边界处的卷积结果，通常会导致输出shape与输入shape相同。 activation：激活函数，为预定义的激活函数名（参考激活函数），或逐元素（element-wise）的Theano函数。如果不指定该参数，将不会使用任何激活函数（即使用线性激活函数：a(x)=x） dilation_rate：单个整数或由两个个整数构成的list/tuple，指定dilated convolution中的膨胀比例。任何不为1的dilation_rate均与任何不为1的strides均不兼容。 data_format：字符串，“channels_first”或“channels_last”之一，代表图像的通道维的位置。该参数是Keras 1.x中的image_dim_ordering，“channels_last”对应原本的“tf”，“channels_first”对应原本的“th”。以128x128的RGB图像为例，“channels_first”应将数据组织为（3,128,128），而“channels_last”应将数据组织为（128,128,3）。该参数的默认值是~/.keras/keras.json中设置的值，若从未设置过，则为“channels_last”。 use_bias:布尔值，是否使用偏置项 kernel_initializer：权值初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。参考initializers bias_initializer：权值初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。参考initializers kernel_regularizer：施加在权重上的正则项，为Regularizer对象 bias_regularizer：施加在偏置向量上的正则项，为Regularizer对象 activity_regularizer：施加在输出上的正则项，为Regularizer对象 kernel_constraints：施加在权重上的约束项，为Constraints对象 bias_constraints：施加在偏置上的约束项，为Constraints对象 详细了解这些参数，建议参阅官方文档。下图表示3*3、stride为1的卷积： 卷积层中的参数数量我们的卷积层中参数数量取决于 filters、kernel_size 和 input_shape 的值。这里定义几个变量： K - 卷积层中的过滤器数量 F - 卷积过滤器的高度和宽度 D_in - 上一层级的深度 注意：K = filters，F = kernel_size。类似地，D_in 是 input_shape 元祖中的最后一个值。因为每个过滤器有 F*F*D_in 个权重，卷积层由 K 个过滤器组成，因此卷积层中的权重总数是 K*F*F*D_in。因为每个过滤器有 1 个偏差项，卷积层有 K 个偏差。因此，卷积层中的参数数量是 K*F*F*D_in+K。 卷积层的形状卷积层的形状取决于 kernel_size、input_shape、padding 和 stride 的值。我们定义几个变量： K - 卷积层中的过滤器数量 F - 卷积过滤器的高度和宽度 H_in - 上一层级的高度 W_in - 上一层级的宽度 注意：K = filters、F = kernel_size，以及S = stride。类似地，H_in 和 W_in 分别是 input_shape 元祖的第一个和第二个值。 卷积层的深度始终为过滤器数量 K。 如果 padding = ‘same’，那么卷积层的空间维度如下： height = ceil(float(H_in) / float(S)) width = ceil(float(W_in) / float(S)) 如果 padding = ‘valid’，那么卷积层的空间维度如下: height = ceil(float(H_in - F + 1) / float(S)) width = ceil(float(W_in - F + 1) / float(S)) 练习1234567from keras.models import Sequentialfrom keras.layers import Conv2Dmodel = Sequential()model.add(Conv2D(filters=32, kernel_size=3, strides=2, padding=&apos;same&apos;, activation=&apos;relu&apos;, input_shape=(128, 128, 3)))model.summary() 习题1该卷积层有多少个参数？A.902B.306C.896D.1034 答案：(32 x 3 x 3 x 3) + 32 = 896 习题2卷积层的深度是多少？A.3B.16C.32D.64 答案：卷积层的深度始终等于过滤器的数量32。 习题2卷积层的宽度是多少？A.3B.16C.32D.64 答案：64 总结卷积神经的维度跟参数密切相关，Conv2D这个接口的参数很多，但是每个参数都有不同的作用。最后总结下通常用的比较多参数： filters - 过滤器数量。过滤器数量和每个过滤器的大小控制卷积层的行为。 kernel_size - 指定（方形）卷积窗口的高和宽的数字，可表示为数字或元组。 你可能还需要调整其他可选参数： strides - 卷积 stride ，指过滤器滑过图片的数量。如果不指定任何值，则 strides 设为 1，可表示为数字或元组。 padding - 选项包括 ‘valid’ 和 ‘same’。如果不指定任何值，则 padding 设为 ‘valid’。 activation - 通常为 ‘relu’。如果未指定任何值，则不应用任何激活函数。强烈建议你向网络中的每个卷积层添加一个 ReLU 激活函数。 在模型中将卷积层当做第一层级（出现在输入层之后）时，必须提供另一个 input_shape 参数： input_shape - 指定输入的高度、宽度和深度（按此顺序）的元组。]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
</search>
