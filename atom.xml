<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Seven&#39;s Blog</title>
  
  <subtitle>Stay Hungry. Stay Foolish.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.seventech.top/"/>
  <updated>2018-07-29T03:31:59.332Z</updated>
  <id>https://www.seventech.top/</id>
  
  <author>
    <name>Seven</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>反向传播（二）</title>
    <link href="https://www.seventech.top/2018/07/26/back-propagation-2/"/>
    <id>https://www.seventech.top/2018/07/26/back-propagation-2/</id>
    <published>2018-07-26T02:27:02.000Z</published>
    <updated>2018-07-29T03:31:59.332Z</updated>
    
    <content type="html"><![CDATA[<p>如何让多层神经网络学习呢？我们已了解了使用梯度下降来更新权重，反向传播算法则是它的一个延伸。要使用梯度下降法更新隐藏层的权重，你需要知道各隐藏层节点的误差对最终输出的影响。每层的输出是由两层间的权重决定的，两层之间产生的误差，按权重缩放后在网络中向前传播。既然我们知道输出误差，便可以用权重来反向传播到隐藏层。<br><a id="more"></a><br>我们已经知道一个权重的更新可以这样计算：</p><script type="math/tex; mode=display">\Delta w_{i} = \eta\delta_j x_i</script><p>这里 $\delta$(error term) 是指：</p><script type="math/tex; mode=display">\delta_j = (y_j-\hat{y_j})f^\prime(h_j) = (y_j-\hat{y_j})f^\prime_j(\sum_{}w_ix_i)</script><p>上面公式中 $(y_j-\hat{y_j})$ 是输出误差，激活函数 $f(h_j)$ 的导函是 $f^\prime(h_j)$ ，我们把这个导函数称做输出的梯度。</p><p>网上反向传播的版本很多，看千百个版本不如自己手推一个版本。<br><img src="/2018/07/26/back-propagation-2/manuscript.jpg" alt="manuscript"></p><h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><ul><li>层 - 用带括号的字母L表示层，(L)表示第L层，(L-1)表示第L-1层，也就是L层的上一层，(L+1)表示第L+1层，也就是L层的下一层。</li><li>下标 - 用 i ，j ， k ，分别表示第 L-1 层的任意节点，第 L 层的任意节点，以及第 L+1 层的任意节点。</li><li>$a^{(L)}_i$ - 表示节点 i 在第 L 层的输出。</li><li>$w^{(L)}_{ij}$ -  表示在节点 i 和节点 j之间的权重，它代表第 L 层的权重。</li><li>$o^{(L)}_i$ - 表示节点 i 在第 L 层的输出。</li><li>$r^{(L)}$ - 表示第 L 层的节点数量</li><li>$b^{(L)}_{i}$ - 表示节点 i 在第 L 层的偏差</li><li>$\sigma()$ - 表示激活函数 sigmoid<br><img src="/2018/07/26/back-propagation-2/cnn_network.png" alt="cnn_network"></li></ul><h1 id="前置条件"><a href="#前置条件" class="headerlink" title="前置条件"></a>前置条件</h1><p>本文的的激活函数不再用 f(x) 表示，而用 sigmoid 函数作为激活函数，表示为 $\sigma()$。</p><p>为了进一步简化，对于节点 j 在第 L 层 的偏差 $b^{(L)}_{j}$ 将被合并到权重中，表示为 $w^{(L)}_{0j}$：</p><script type="math/tex; mode=display">w^{(L)}_{0j} = b^{(L)}_j</script><p>那么节点 j 的输出：</p><script type="math/tex; mode=display">a^{(L)}_j = b^{(L)}_j + \sum^{r^{(L-1)}}_{i=1} w^{(L)}_{ij}o^{(L-1)}_i = \sum^{r^{(L-1)}}_{i=0} w^{(L)}_{ij}o^{(L-1)}_i</script><h1 id="误差函数"><a href="#误差函数" class="headerlink" title="误差函数"></a>误差函数</h1><p>我们依然用 SSE 作为衡量预测结果的标准：</p><script type="math/tex; mode=display">E = \frac{1}{2}(y - \hat{y})^2</script><h1 id="误差偏导"><a href="#误差偏导" class="headerlink" title="误差偏导"></a>误差偏导</h1><p>反向传播算法的推导通过将链式法则应用于误差函数偏导开始：</p><script type="math/tex; mode=display">\frac{\alpha E}{\alpha w^{(L)}_{ij}} = \frac{\alpha E}{\alpha a^{(L)}_{j}}\frac{\alpha a^{(L)}_j}{\alpha w^{(L)}_{ij}}</script><p>误差项依然用 $\delta$ 表示，不同的是带了上下标，表示节点 j 在第 L 层的误差项：</p><script type="math/tex; mode=display">\delta^{(L)}_j =\frac{\alpha E}{\alpha a^{(L)}_{j}}</script><p>而</p><script type="math/tex; mode=display">a^{(L)}_j = \sum^{r^{(L-1)}}_{i=0}w^{(L)}_{ij}o^{(L-1)}_i</script><p>$o^{(L-1)}_i$ 表示节点 i 在第 L-1 层的输出，求偏导可知：</p><script type="math/tex; mode=display">\frac{\alpha a^{(L)}_j}{\alpha w^{(L)}_{ij}} = o^{(L-1)}_i</script><p>因此</p><script type="math/tex; mode=display">\frac{\alpha E}{\alpha w^{(L)}_{ij}} = \delta^{(L)}_jo^{(L-1)}_i</script><h1 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h1><p>这里我们假设输出层只有一个节点，因此节点的下标是1而不是 k ，那么最后一层的输出表示为$a^{(L+1)}_1$，由误差函数可知：</p><script type="math/tex; mode=display">E = \frac{1}{2}(y - \hat{y})^2 = \frac{1}{2}(y-\sigma(a^{(L+1)}_1))^2</script><p>$\sigma()$ 是激活函数 sigmoid 函数。同样令输出层的误差项为 $\delta^{(L+1)}_1 = \frac{\alpha E}{\alpha a^{(L+1)}_{1}}$，所以</p><script type="math/tex; mode=display">\delta^{(L+1)}_1 = \frac{\alpha E}{\alpha a^{(L+1)}_{1}} =  -(y-\sigma(a^{(L+1)}_1))\sigma^\prime(a^{(L+1)}_1) = -(y-\hat{y})\sigma^\prime(a^{(L+1)}_1)</script><h1 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h1><p>对于隐藏层节点 j 的误差项：</p><script type="math/tex; mode=display">\delta^{(L)}_j = \frac{\alpha E}{\alpha a^{(L)}_{j}} = \sum^{r^{(L+1)}}_{k=1}\frac{\alpha E}{\alpha a^{(L+1)}_{k}}\frac{\alpha a^{(L+1)}_k}{\alpha a^{(L)}_j}</script><p>上式公式中为什么要求和？请看下图：<br><img src="/2018/07/26/back-propagation-2/sum_nodes.png" alt="sum_nodes"><br>因为激活值可以通过不同的途径影响代价函数(误差函数)，也就是说神经元一边通过 $a^{(L+1)}_1$ 来影响代价函数，另一边通过 $a^{(L+1)}_2$ 来影响代价函数，得把这些都加起来。至于求和公式中 k 是从1开始的，因为输出层没有偏差，只有输出节点，而从0开始表示该层包含一个偏差的节点。</p><p>令 $\delta^{(L+1)}_k = \frac{\alpha E}{\alpha a^{(L+1)}_{k}}$ ，所以</p><script type="math/tex; mode=display">\delta^{(L)}_j = \sum^{r^{(L+1)}}_{k=1}\delta^{(L+1)}_k\frac{\alpha a^{(L+1)}_k}{\alpha a^{(L)}_j}</script><p>由于</p><script type="math/tex; mode=display">a^{(L+1)}_k = \sum^{r^{(L)}}_{j=0}w^{(L+1)}_{jk}\sigma(a^{(L)}_j)</script><p>所以</p><script type="math/tex; mode=display">\frac{\alpha a^{(L+1)}_k}{\alpha a^{(L)}_j} = w^{(L+1)}_{jk}\sigma^\prime(a^{(L)}_j)</script><p>因此</p><script type="math/tex; mode=display">\delta^{(L)}_j = \sum^{r^{(L+1)}}_{k=1}\delta^{(L+1)}_kw^{(L+1)}_{jk}\sigma^\prime(a^{(L)}_j) = \sigma^\prime(a^{(L)}_j)\sum^{r^{(L+1)}}_{k=1}w^{(L+1)}_{jk}\delta^{(L+1)}_k</script><p>最后带入公式</p><script type="math/tex; mode=display">\frac{\alpha E}{\alpha w^{(L)}_{ij}} = \delta^{(L)}_jo^{(L-1)}_i = \sigma^\prime(a^{(L)}_j)o^{(L-1)}_i\sum^{r^{(L+1)}}_{k=1}w^{(L+1)}_{jk}\delta^{(L+1)}_k</script><p>至此，反向传播推导就结束了。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>上面推导出了很多公式，总结一下对我们有用的公式：</p><ul><li><code>误差函数对权重的偏导，可以理解为代价函数（误差）对权重 w 的微小变化有多敏感</code><br><strong><script type="math/tex">\frac{\alpha E}{\alpha w^{(L)}_{ij}} = \delta^{(L)}_jo^{(L-1)}_i</script></strong></li><li><code>输出层的误差项</code><br><strong><script type="math/tex">\delta^{(L+1)}_1 =  -(y-\hat{y})\sigma^\prime(a^{(L+1)}_1)</script></strong></li><li><code>隐藏层误差项</code><br><strong><script type="math/tex">\delta^{(L)}_j = \sigma^\prime(a^{(L)}_j)\sum^{r^{(L+1)}}_{k=1}w^{(L+1)}_{jk}\delta^{(L+1)}_k</script></strong></li></ul><h1 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from data_prep import features, targets, features_test, targets_test</span><br><span class="line"></span><br><span class="line">np.random.seed(21)</span><br><span class="line"></span><br><span class="line">def sigmoid(x):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Calculate sigmoid</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return 1 / (1 + np.exp(-x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 超参数</span><br><span class="line">n_hidden = 2  # number of hidden units</span><br><span class="line">epochs = 900</span><br><span class="line">learnrate = 0.005</span><br><span class="line"></span><br><span class="line">n_records, n_features = features.shape</span><br><span class="line">last_loss = None</span><br><span class="line"># 初始化权重</span><br><span class="line">weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,</span><br><span class="line">                                        size=(n_features, n_hidden))</span><br><span class="line">weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,</span><br><span class="line">                                         size=n_hidden)</span><br><span class="line">print(&apos;weights_input_hidden=&#123;&#125;&apos;.format(weights_input_hidden))</span><br><span class="line">print(&apos;weights_hidden_output=&#123;&#125;&apos;.format(weights_hidden_output))</span><br><span class="line">for e in range(epochs):</span><br><span class="line">    del_w_input_hidden = np.zeros(weights_input_hidden.shape)</span><br><span class="line">    del_w_hidden_output = np.zeros(weights_hidden_output.shape)</span><br><span class="line"></span><br><span class="line">    for x, y in zip(features.values, targets):</span><br><span class="line">        ## 前向传播 ##</span><br><span class="line">        # TODO: 计算输出</span><br><span class="line">        #print(&apos;x=&#123;&#125;&apos;.format(x))</span><br><span class="line">        hidden_input = np.dot(x, weights_input_hidden)</span><br><span class="line">        hidden_output = sigmoid(hidden_input)</span><br><span class="line">        output = sigmoid(np.dot(hidden_output, weights_hidden_output))</span><br><span class="line"></span><br><span class="line">        ## 后向传播 ##</span><br><span class="line">        # TODO: 实际值与期望的差值</span><br><span class="line">        error = y - output</span><br><span class="line"></span><br><span class="line">        # TODO: 对输出求误差梯度</span><br><span class="line">        output_error_term = error * output * (1-output)</span><br><span class="line"></span><br><span class="line">        ## 传播误差到隐藏层</span><br><span class="line"></span><br><span class="line">        # TODO: 计算隐藏层对误差的贡献</span><br><span class="line">        hidden_error = np.dot(weights_hidden_output, output_error_term)</span><br><span class="line"></span><br><span class="line">        # TODO: 对隐藏层求误差梯度</span><br><span class="line">        hidden_error_term = hidden_error * hidden_output * (1-hidden_output)</span><br><span class="line"></span><br><span class="line">        # TODO: 更新权重的变化率</span><br><span class="line">        del_w_hidden_output += output_error_term * hidden_output</span><br><span class="line">        del_w_input_hidden += hidden_error_term * x[:, None]</span><br><span class="line"></span><br><span class="line">    # TODO: 更新权重</span><br><span class="line">    weights_input_hidden += learnrate * del_w_input_hidden / n_records</span><br><span class="line">    weights_hidden_output += learnrate * del_w_hidden_output / n_records</span><br><span class="line"></span><br><span class="line">    # 打印训练集上的均方差</span><br><span class="line">    if e % (epochs / 10) == 0:</span><br><span class="line">        hidden_output = sigmoid(np.dot(x, weights_input_hidden))</span><br><span class="line">        out = sigmoid(np.dot(hidden_output,</span><br><span class="line">                             weights_hidden_output))</span><br><span class="line">        loss = np.mean((out - targets) ** 2)</span><br><span class="line"></span><br><span class="line">        if last_loss and last_loss &lt; loss:</span><br><span class="line">            print(&quot;Train loss: &quot;, loss, &quot;  WARNING - Loss Increasing&quot;)</span><br><span class="line">        else:</span><br><span class="line">            print(&quot;Train loss: &quot;, loss)</span><br><span class="line">        last_loss = loss</span><br><span class="line"></span><br><span class="line"># 对测试数据计算准确度</span><br><span class="line">hidden = sigmoid(np.dot(features_test, weights_input_hidden))</span><br><span class="line">out = sigmoid(np.dot(hidden, weights_hidden_output))</span><br><span class="line">predictions = out &gt; 0.5</span><br><span class="line">accuracy = np.mean(predictions == targets_test)</span><br><span class="line">print(&quot;Prediction accuracy: &#123;:.3f&#125;&quot;.format(accuracy))</span><br></pre></td></tr></table></figure><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul><li><a href="https://brilliant.org/wiki/backpropagation/" target="_blank" rel="noopener">Brilliant-Backpropagation</a></li><li><a href="https://www.bilibili.com/video/av16577449/?p=2" target="_blank" rel="noopener">反向传播的微积分原理</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;如何让多层神经网络学习呢？我们已了解了使用梯度下降来更新权重，反向传播算法则是它的一个延伸。要使用梯度下降法更新隐藏层的权重，你需要知道各隐藏层节点的误差对最终输出的影响。每层的输出是由两层间的权重决定的，两层之间产生的误差，按权重缩放后在网络中向前传播。既然我们知道输出误差，便可以用权重来反向传播到隐藏层。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="https://www.seventech.top/categories/AI/"/>
    
    
      <category term="CNN" scheme="https://www.seventech.top/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>反向传播（一）</title>
    <link href="https://www.seventech.top/2018/07/20/back-propagation-1/"/>
    <id>https://www.seventech.top/2018/07/20/back-propagation-1/</id>
    <published>2018-07-20T03:57:34.000Z</published>
    <updated>2018-07-27T11:41:26.423Z</updated>
    
    <content type="html"><![CDATA[<p>反向传播是神经网络的基石，它的名字也很容易理解，因为梯度的计算是通过网络向后进行，首先计算最终权重层的梯度，并且最后计算第一层权重的梯度。在计算前一层梯度时，重复使用来自上一层梯度的计算部分。这种误差信息的向后流动就形象的称为反向传播。<br><a id="more"></a></p><h1 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h1><p>首先介绍下链式法则，他是反向传播的核心，链式法则也即复合函数求导的过程。<br><img src="/2018/07/20/back-propagation-1/chain_rule.png" alt="chain_rule"><br>链式法则的内容是：如果有一个变量 x ，以及一个关于 x 的函数 f(x) 我们将简称为 A ，另一个函数 g ，将 f(x) 作为 g 的变量得到 g o f(x) 。链式法则证明： B 关于 x 的偏导数，就是 B 关于 A 的偏导数，乘以 A 关于 x 的偏导数，所以对复合函数求导就是一系列导数的乘积。</p><h1 id="SSE"><a href="#SSE" class="headerlink" title="SSE"></a>SSE</h1><p>怎么样衡量预测结果的标准？最简单，最容易想到的是用实际目标值 y 减去网络输出值 ŷ ，以两者差值衡量误差。</p><script type="math/tex; mode=display">E = y - \hat{y}</script><p>然而，若预测值高于目标值，那么差值就为负数，若预测值低于目标值，差值为正数，我们希望误差能够保持符号一致，要让符号全部归正，可以求差值的平方：</p><script type="math/tex; mode=display">E = (y - \hat{y})^2</script><p>你可能在想，为什么不直接用绝对值呢？问得好！这是因为<strong>使用平方值时，异常数值会被赋予更高的惩罚值，而较小误差的惩罚值则较低</strong>。</p><p>目前我们仅得到单次预测的误差，我们希望求出全体数据的整体误差，那么可以对每项数据 <strong>μ</strong> 的误差求和:</p><script type="math/tex; mode=display">E = \sum_{μ}(y - \hat{y})^2</script><p>最后我们在式子前面加上$\frac{1}{2}$，以便简化后续计算。怎么个简化法？这是为了后面求导时可以跟平方数抵消，那么最后的公式：</p><script type="math/tex; mode=display">E = \frac{1}{2}\sum_{μ}(y - \hat{y})^2</script><p>此公式通常称为误差平方和，简称 SSE(sum of the squared errors)。误差平方和，可用于衡量神经网络的预测效果，值越高，预测效果越差，值越低，预测效果越好，这也是刚才我们求完差值后为什么用平方而不是取绝对值的原因。</p><h1 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h1><p>这里举一个简化的例子来阐述梯度下降的过程，我们只考虑单行数据的情况。<br><img src="/2018/07/20/back-propagation-1/simple_back_propagation.png" alt="simple_back_propagation"><br>上面SSE的公式中全体数据用 <strong>μ</strong> 表示，我们可以将这些数据看成两组表格、数组或者矩阵,其中一组包含输入值，另一组包含目标值 y ，每项数据占一行位置 μ = 1 表示第一行数据，如果需要计算整体误差，可以逐行计算误差平方和，然后对所得结果求和。<br><img src="/2018/07/20/back-propagation-1/one_raw_data.png" alt="one_raw_data"><br>我们将数据带入到公式中：</p><script type="math/tex; mode=display">E = \frac{1}{2}(y - \hat{y})^2  = \frac{1}{2}(y - f(\sum_{i}w_ix_i))^2</script><p>展开预测值之后，可以看出权重是误差函数的参数，我们的目标是求取能使误差最小化的权重值，如下图：<br><img src="/2018/07/20/back-propagation-1/error_function.png" alt="error_function"><br>由上图知，我们的目标就是求取图形碗底对应的权值，从某个随机权值出发，逐步向误差最小值的方向前进，这个方向与梯度（斜率）相反，只要始终沿着梯度反复逐步下降，最终能求得对应最小误差的权值，这就是梯度下降的过程。<br><img src="/2018/07/20/back-propagation-1/gradient_descent.png" alt="gradient_descent"><br>下面来更新权值，新的权值 $w_i$ 等于旧的权值 $w_i$ 加上更新步长$\Delta w_i$：</p><script type="math/tex; mode=display">w_i = w_i + \Delta w_i</script><p>由于前述更新步长与梯度成正比，而梯度等于误差关于每个权重$w_i$的偏导数：</p><script type="math/tex; mode=display">\Delta w_i \propto -\frac{\alpha E}{\alpha w_i}</script><p>公式中还需添加一个缩放系数变量用来控制梯度下降中更新步长的大小：</p><script type="math/tex; mode=display">\Delta w_i = -\eta\frac{\alpha E}{\alpha w_i}</script><p>这个变量叫作学习速率用希腊字母 $\eta$ 表示。对于计算梯度需要用到多元微积分，也就是前面提到的链式法则。下面我们展开计算梯度：</p><script type="math/tex; mode=display">\frac{\alpha E}{\alpha w_i} = \frac{\alpha}{\alpha w_i}\frac{1}{2}(y-\hat{y})^2= \frac{\alpha}{\alpha w_i}\frac{1}{2}(y-\hat{y}(w_i))^2</script><p>鉴于输出值 $\hat{y}$ 是权重的函数，这里相当于复合函数，可以通过链式法则来求偏导，前面已经介绍过了，那么：</p><script type="math/tex; mode=display">\frac{\alpha E}{\alpha w_i} = (y-\hat{y})\frac{\alpha}{\alpha w_i}(y-\hat{y})= -(y-\hat{y})\frac{\alpha \hat{y}}{\alpha w_i}</script><p>下面再来对 $\hat{y}$ 求偏导，我们知道</p><script type="math/tex; mode=display">\hat{y} =f(h)</script><p>而</p><script type="math/tex; mode=display">h = \sum_{i}w_ix_i</script><p>因此</p><script type="math/tex; mode=display">\frac{\alpha E}{\alpha w_i} = -(y-\hat{y})f^\prime(h)\frac{\alpha}{\alpha w_i}\sum_{i}w_ix_i</script><p>我们把上式的最后一项单独提出来，在求和式子中，每个权重仅是单个子项的参数：</p><script type="math/tex; mode=display">\frac{\alpha}{\alpha w_i}\sum_{i}w_ix_i</script><p>将其展开</p><script type="math/tex; mode=display">\frac{\alpha}{\alpha w_i}\sum_{i}w_ix_i = \frac{\alpha}{\alpha w_i}[w_1x_1+w_2x_2+...+w_nx_n]= x_i+0+0+0+...</script><p>综合来看，误差平方关于 $w_i$ 的偏导数等于<br><strong><script type="math/tex">\frac{\alpha E}{\alpha w_i} = -(y-\hat{y})f^\prime(h)x_i</script></strong><br>所以</p><script type="math/tex; mode=display">\Delta w_i = \eta(y-\hat{y})f^\prime(h)x_i</script><p>最后我们得出结论，更新步长就等于学习速率 $\eta$ 乘以预测差值，乘以激活函数导数，再乘以输入值，为方便后续应用，我们将预测差值乘以激活函数的导数用小写字母 $\delta$ 表示</p><script type="math/tex; mode=display">\delta = (y-\hat{y})f^\prime(h)</script><p>那么权值更新公式可以重新写为<br><strong><script type="math/tex">w_i = w_i+\eta\delta x_i</script></strong><br>以上便是单行数据的情况，你的神经网络中可能会有多个输出单元，可以将其视为单个输出网络的堆叠架构，但需将输入单元连接到新的输出单元，这时整体误差等于每个输出单元的误差之和。<br><img src="/2018/07/20/back-propagation-1/multi_back_propagation.png" alt="multi_back_propagation"><br>梯度下降法可以扩展适用于这种情况，只需分别计算每个输出单元的误差项<br><strong><script type="math/tex">\delta_j = (y_j-\hat{y}_j)f^\prime(h_j)</script></strong><br><strong><script type="math/tex">\Delta w_{ij} = \eta\delta_jx_i</script></strong><br>以上便是反向传播的核心知识点，下一章，对反向传播进行数学推导。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;反向传播是神经网络的基石，它的名字也很容易理解，因为梯度的计算是通过网络向后进行，首先计算最终权重层的梯度，并且最后计算第一层权重的梯度。在计算前一层梯度时，重复使用来自上一层梯度的计算部分。这种误差信息的向后流动就形象的称为反向传播。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="https://www.seventech.top/categories/AI/"/>
    
    
      <category term="CNN" scheme="https://www.seventech.top/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>前向传播</title>
    <link href="https://www.seventech.top/2018/07/20/forward-propagation/"/>
    <id>https://www.seventech.top/2018/07/20/forward-propagation/</id>
    <published>2018-07-20T02:33:45.000Z</published>
    <updated>2018-07-27T11:41:50.054Z</updated>
    
    <content type="html"><![CDATA[<p>前向传播是神经网络用来将输入变成输出的流程。<br><a id="more"></a></p><h1 id="输入层-gt-隐藏层"><a href="#输入层-gt-隐藏层" class="headerlink" title="输入层-&gt;隐藏层"></a>输入层-&gt;隐藏层</h1><p>在下面这个简单的网络图中，输入单元被标注为 $x_{_1}$​，$x_{_2}$​，$x_{_3}$​，隐藏层节点是 $h_{_1}$​，$h_{_2}$。<br><img src="http://pbzo8ekj5.bkt.clouddn.com/3_inputs_2_hides.png" alt="3_inputs_2_hides"><br>这个网络图中有多个输入单元和多个隐藏单元，它们的权重需要有两个索引 $w_{_{ij}}$​，其中 i 表示输入单元，j 表示隐藏单元。</p><p>为了定位权重，我们把输入节点的索引 i 和隐藏节点的索引 j​ 结合，得到：<br>$w_{_{11}}$ - 代表从 $x_{_1}$​ 到 $h_{_1}$​ 的权重；<br>$w_{_{12}}$ - 代表从 $x_{_1}$ 到 $h_{_2}$ 的权重。<br>…</p><p>下图包括了从输入层到隐藏层的所有权重，用 $w_{_{ij}}$​ 表示：<br><img src="/2018/07/20/forward-propagation/3_inputs_2_hides_1.png" alt="3_inputs_2_hides_1"><br>现在，将权重储存在矩阵中，由 $w_{_{ij}}$​ 来索引。矩阵中的每一行对应从同一个输入节点发出的权重，每一列对应传入同一个隐藏节点的权重。这里我们有三个输入节点，两个隐藏节点，权重矩阵表示为：<br><img src="/2018/07/20/forward-propagation/3_inputs_2_hides_weight_matrix.png" alt="3_inputs_2_hides_weight_matrix"><br>运用矩阵乘法可以得出每一个隐藏层节点 $h_{_j}$：</p><script type="math/tex; mode=display">h_{_j} = \sum_{i}x_{_i}w_{_{ij}}</script><p>那么：</p><script type="math/tex; mode=display">h_{_1} = x_{_1}w_{_{11}} + x_{_2}w_{_{21}} + x_{_3}w_{_{31}}</script><p>以此类推，可以求出第二个隐藏节点$h_{_2}$：</p><script type="math/tex; mode=display">h_{_2} = x_{_1}w_{_{12}} + x_{_2}w_{_{22}} + x_{_3}w_{_{32}}</script><p>在 NumPy 中，我们可以直接使用 np.dot：<br><code>hidden_inputs = np.dot(inputs, weights_input_to_hidden)</code></p><p>上面的网络图是不含偏差的，实际上加上偏差的网络图前向传播是类似的:<br><img src="/2018/07/20/forward-propagation/feedforward.png" alt="feedforward"><br>如上图具有上标(1)的权重属于第一层，具有上标(2)的权重属于第二层级，偏差不再叫b，现在叫做 $W_{31}$， $W_{_{32}}$ 等等。</p><p>在第一层级中，我们执行前向传播可以得出：</p><script type="math/tex; mode=display">h_1 = W^{^{(1)}}_{_{11}}x_{_1} + W^{^{(1)}}_{_{21}}x_{_2} + W^{^{(1)}}_{_{31}}</script><script type="math/tex; mode=display">h_2 = W^{^{(1)}}_{_{12}}x_{_1} + W^{^{(1)}}_{_{22}}x_{_2} + W^{^{(1)}}_{_{32}}</script><p>我们再给每层加上一个激活函数，选定 sigmoid 函数为我们的激活函数。先简单介绍下 sigmoid 函数。</p><h2 id="sigmoid-函数"><a href="#sigmoid-函数" class="headerlink" title="sigmoid 函数"></a>sigmoid 函数</h2><p>sigmoid 函数是一个有着优美S形曲线的数学函数，在逻辑回归、人工神经网络中有着广泛的应用。它的数学形式：</p><script type="math/tex; mode=display">\sigma(x) = \frac{1}{1+e^{-x}}</script><p>其函数图像如下：<br><img src="/2018/07/20/forward-propagation/sigmoid.png" alt="sigmoid"><br>可以看出，sigmoid 函数连续，光滑，严格单调，以(0,0.5)中心对称，是一个非常良好的阈值函数。sigmoid 函数的值域范围限制在(0,1)之间，我们知道[0,1]与概率值的范围是相对应的，这样sigmoid函数就能与一个概率分布联系起来了。</p><h2 id="sigmoid-函数的导数"><a href="#sigmoid-函数的导数" class="headerlink" title="sigmoid 函数的导数"></a>sigmoid 函数的导数</h2><p>sigmoid 函数有一个美丽的导数，我们可以在下面的计算中看到。这将使我们的反向传播步骤更加简洁，这也是这里为什么选用它为激活函数的原因。</p><script type="math/tex; mode=display">\sigma^{'}(x) = \frac{\alpha}{\alpha x}\frac{1}{1+e^{-x}}              = \frac{e^{-x}}{(1+e^{-x})^2}              = \frac{1}{1+e^{-x}}\frac{e^{-x}}{1+e^{-x}}              = \sigma(x)(1-\sigma(x))</script><p>隐藏层节点通过 sigmoid 激活函数可以分别求得隐藏层的输出为:<br>hide_out_1 = $\sigma(h_1)$<br>hide_out_2 = $\sigma(h_2)$</p><h1 id="隐藏层-gt-输出层"><a href="#隐藏层-gt-输出层" class="headerlink" title="隐藏层-&gt;输出层"></a>隐藏层-&gt;输出层</h1><p>通过线性方程，结合隐藏层的输出可以求出输出节点：</p><script type="math/tex; mode=display">h = W^{^{(2)}}_{_{11}}\sigma(h_1) + W^{^{(2)}}_{_{21}}\sigma(h_2) + W^{^{(2)}}_{_{31}}</script><p>对输出节点应用 sigmoid 函数，得出预测：</p><script type="math/tex; mode=display">\hat{y} = \sigma(h)</script><p>即在 0 到 1 之间的概率 用 ŷ 表示。至此，前向传播就结束了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前向传播是神经网络用来将输入变成输出的流程。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="https://www.seventech.top/categories/AI/"/>
    
    
      <category term="CNN" scheme="https://www.seventech.top/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>CS231n课程中文笔记汇总</title>
    <link href="https://www.seventech.top/2018/07/18/CS231n-notes/"/>
    <id>https://www.seventech.top/2018/07/18/CS231n-notes/</id>
    <published>2018-07-18T03:10:26.000Z</published>
    <updated>2018-07-27T11:42:49.698Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CS231n简介"><a href="#CS231n简介" class="headerlink" title="CS231n简介"></a>CS231n简介</h1><p>CS231n的全称是CS231n: Convolutional Neural Networks for Visual Recognition，即面向视觉识别的卷积神经网络。该课程是斯坦福大学计算机视觉实验室推出的课程。需要注意的是，目前大家说CS231n，大都指的是2016年冬季学期（一月到三月）的最新版本。</p><p>对于不喜欢看英文笔记的同学，本文收集了CS231n课程的中文笔记。笔记来源：智能单元 - 知乎专栏。<br><a id="more"></a></p><h1 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h1><p><a href="https://zhuanlan.zhihu.com/p/21930884" target="_blank" rel="noopener">CS231n官方笔记授权翻译总集篇</a></p><p><a href="https://zhuanlan.zhihu.com/p/20878530?refer=intelligentunit" target="_blank" rel="noopener">Python Numpy教程</a></p><p><a href="https://zhuanlan.zhihu.com/p/20894041" target="_blank" rel="noopener">图像分类笔记（上）</a></p><p><a href="https://zhuanlan.zhihu.com/p/20900216" target="_blank" rel="noopener">图像分类笔记（下）</a></p><p><a href="https://zhuanlan.zhihu.com/p/20918580" target="_blank" rel="noopener">线性分类笔记（上）</a></p><p><a href="https://zhuanlan.zhihu.com/p/20945670" target="_blank" rel="noopener">线性分类笔记（中）</a></p><p><a href="https://zhuanlan.zhihu.com/p/21102293" target="_blank" rel="noopener">线性分类笔记（下）</a></p><p><a href="https://zhuanlan.zhihu.com/p/21360434" target="_blank" rel="noopener">最优化笔记（上）</a></p><p><a href="https://zhuanlan.zhihu.com/p/21387326" target="_blank" rel="noopener">最优化笔记（下）</a></p><p><a href="https://zhuanlan.zhihu.com/p/21407711" target="_blank" rel="noopener">反向传播笔记</a></p><p><a href="https://zhuanlan.zhihu.com/p/21462488" target="_blank" rel="noopener">神经网络笔记1（上）</a></p><p><a href="https://zhuanlan.zhihu.com/p/21513367" target="_blank" rel="noopener">神经网络笔记1（下）</a></p><p><a href="https://zhuanlan.zhihu.com/p/21560667" target="_blank" rel="noopener">神经网络笔记 2</a></p><p><a href="https://zhuanlan.zhihu.com/p/21741716" target="_blank" rel="noopener">神经网络笔记3（上）</a></p><p><a href="https://zhuanlan.zhihu.com/p/21798784" target="_blank" rel="noopener">神经网络笔记3（下）</a></p><p><a href="https://zhuanlan.zhihu.com/p/22038289" target="_blank" rel="noopener">卷积神经网络笔记</a></p><p><a href="https://zhuanlan.zhihu.com/p/21441838" target="_blank" rel="noopener">课程作业# 1简介</a></p><p><a href="https://zhuanlan.zhihu.com/p/21941485" target="_blank" rel="noopener">课程作业# 2简介</a></p><p><a href="https://zhuanlan.zhihu.com/p/21946525" target="_blank" rel="noopener">课程作业# 3简介</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;CS231n简介&quot;&gt;&lt;a href=&quot;#CS231n简介&quot; class=&quot;headerlink&quot; title=&quot;CS231n简介&quot;&gt;&lt;/a&gt;CS231n简介&lt;/h1&gt;&lt;p&gt;CS231n的全称是CS231n: Convolutional Neural Networks for Visual Recognition，即面向视觉识别的卷积神经网络。该课程是斯坦福大学计算机视觉实验室推出的课程。需要注意的是，目前大家说CS231n，大都指的是2016年冬季学期（一月到三月）的最新版本。&lt;/p&gt;
&lt;p&gt;对于不喜欢看英文笔记的同学，本文收集了CS231n课程的中文笔记。笔记来源：智能单元 - 知乎专栏。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="https://www.seventech.top/categories/AI/"/>
    
    
      <category term="CNN" scheme="https://www.seventech.top/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络中的维度</title>
    <link href="https://www.seventech.top/2018/07/12/cnn-dimension/"/>
    <id>https://www.seventech.top/2018/07/12/cnn-dimension/</id>
    <published>2018-07-12T02:46:32.000Z</published>
    <updated>2018-07-27T11:42:27.152Z</updated>
    
    <content type="html"><![CDATA[<p>神经网络的维度是个经常让人迷糊的概念，本文以卷积神经网络为例介绍怎么计算维度。文中的公式也可以作为知识点backup。<br><a id="more"></a></p><h1 id="维度"><a href="#维度" class="headerlink" title="维度"></a>维度</h1><p>先来个例子，按以下步骤用<strong>Keras</strong>创建一个简单的<strong>CNN</strong>。</p><p>首先创建一个序列模型，使用<strong>add()</strong> 方法向网络中添加层级：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from keras.models import Sequential</span><br><span class="line">from keras.layers import Conv2D</span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Conv2D(filters=16, kernel_size=2, strides=2, padding=&apos;valid&apos;,</span><br><span class="line">    activation=&apos;relu&apos;, input_shape=(200, 200, 1)))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure></p><p>让我们通过该网络所提供的参数研究卷积层的维度如何变化，你可以在jupyter notebook上运行。现在来看看输出结果是什么：</p><p><img src="/2018/07/12/cnn-dimension/summary_result.png" alt="summary_result"><br>结果显示该卷积层有80个参数，对应的输出在<strong>Param #</strong>。同时注意卷积层的形状是如何变化，对应输出内容中的<strong>Output Shape</strong>,上图中，<strong>None</strong>对应的是批次大小，卷积层的高度为<strong>100</strong>，宽度为<strong>100</strong>，深度为<strong>16</strong>。</p><p>本文以<strong>Keras</strong>来搭建卷积网络，让我们先来了解下卷积层（Conv2D）的参数。</p><h1 id="Conv2D"><a href="#Conv2D" class="headerlink" title="Conv2D"></a>Conv2D</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.Conv2D(filters,</span><br><span class="line">                                  kernel_size,</span><br><span class="line">                                  strides=(1, 1),</span><br><span class="line">                                  padding=&apos;valid&apos;,</span><br><span class="line">                                  data_format=None,</span><br><span class="line">                                  dilation_rate=(1, 1),</span><br><span class="line">                                  activation=None,</span><br><span class="line">                                  use_bias=True,</span><br><span class="line">                                  kernel_initializer=&apos;glorot_uniform&apos;,</span><br><span class="line">                                  bias_initializer=&apos;zeros&apos;,</span><br><span class="line">                                  kernel_regularizer=None,</span><br><span class="line">                                  bias_regularizer=None,</span><br><span class="line">                                  activity_regularizer=None,</span><br><span class="line">                                  kernel_constraint=None,</span><br><span class="line">                                  bias_constraint=None)</span><br></pre></td></tr></table></figure><p>二维卷积层，即对图像的空域卷积。该层对二维输入进行滑动窗卷积，当使用该层作为第一层时，应提供input_shape参数。例如input_shape = (128,128,3)代表128*128的彩色RGB图像（data_format=’channels_last’）</p><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><ul><li>filters：卷积核的数目（即输出的维度）</li><li>kernel_size：单个整数或由两个整数构成的list/tuple，卷积核的宽度和长度。如为单个整数，则表示在各个空间维度的相同长度。</li><li>strides：单个整数或由两个整数构成的list/tuple，为卷积的步长。如为单个整数，则表示在各个空间维度的相同步长。任何不为1的strides均与任何不为1的dilation_rate均不兼容</li><li>padding：补0策略，为“valid”, “same” 。“valid”代表只进行有效的卷积，即对边界数据不处理。“same”代表保留边界处的卷积结果，通常会导致输出shape与输入shape相同。</li><li>activation：激活函数，为预定义的激活函数名（参考激活函数），或逐元素（element-wise）的Theano函数。如果不指定该参数，将不会使用任何激活函数（即使用线性激活函数：a(x)=x）</li><li>dilation_rate：单个整数或由两个个整数构成的list/tuple，指定dilated convolution中的膨胀比例。任何不为1的dilation_rate均与任何不为1的strides均不兼容。</li><li>data_format：字符串，“channels_first”或“channels_last”之一，代表图像的通道维的位置。该参数是Keras 1.x中的image_dim_ordering，“channels_last”对应原本的“tf”，“channels_first”对应原本的“th”。以128x128的RGB图像为例，“channels_first”应将数据组织为（3,128,128），而“channels_last”应将数据组织为（128,128,3）。该参数的默认值是~/.keras/keras.json中设置的值，若从未设置过，则为“channels_last”。</li><li>use_bias:布尔值，是否使用偏置项</li><li>kernel_initializer：权值初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。参考initializers</li><li>bias_initializer：权值初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。参考initializers</li><li>kernel_regularizer：施加在权重上的正则项，为Regularizer对象</li><li>bias_regularizer：施加在偏置向量上的正则项，为Regularizer对象</li><li>activity_regularizer：施加在输出上的正则项，为Regularizer对象</li><li>kernel_constraints：施加在权重上的约束项，为Constraints对象</li><li>bias_constraints：施加在偏置上的约束项，为Constraints对象</li></ul><p>详细了解这些参数，建议参阅<a href="https://keras.io/layers/convolutional/" target="_blank" rel="noopener">官方文档</a>。下图表示3*3、stride为1的卷积：<br><img src="/2018/07/12/cnn-dimension/conv2D.gif" alt="conv2D" title="窗口为 3x3、stride 为 1 的卷积"></p><h1 id="卷积层中的参数数量"><a href="#卷积层中的参数数量" class="headerlink" title="卷积层中的参数数量"></a>卷积层中的参数数量</h1><p>我们的卷积层中参数数量取决于 <strong>filters</strong>、<strong>kernel_size</strong> 和 <strong>input_shape</strong> 的值。这里定义几个变量：</p><ul><li><strong>K</strong> - 卷积层中的过滤器数量</li><li><strong>F</strong> - 卷积过滤器的高度和宽度</li><li><strong>D_in</strong> - 上一层级的深度</li></ul><p>注意：<strong>K = filters</strong>，<strong>F = kernel_size</strong>。类似地，<strong>D_in</strong> 是 <strong>input_shape</strong> 元祖中的最后一个值。<br>因为每个过滤器有 <strong>F*F*D_in</strong> 个权重，卷积层由 <strong>K</strong> 个过滤器组成，因此卷积层中的权重总数是 <strong>K*F*F*D_in</strong>。因为每个过滤器有 <strong>1</strong> 个偏差项，卷积层有 <strong>K</strong> 个偏差。因此，卷积层中的参数数量是 <strong>K*F*F*D_in+K</strong>。</p><h1 id="卷积层的形状"><a href="#卷积层的形状" class="headerlink" title="卷积层的形状"></a>卷积层的形状</h1><p>卷积层的形状取决于 <strong>kernel_size</strong>、<strong>input_shape</strong>、<strong>padding</strong> 和 <strong>stride</strong> 的值。我们定义几个变量：</p><ul><li><strong>K</strong> - 卷积层中的过滤器数量</li><li><strong>F</strong> - 卷积过滤器的高度和宽度</li><li><strong>H_in</strong> - 上一层级的高度</li><li><strong>W_in</strong> - 上一层级的宽度</li></ul><p>注意：<strong>K = filters</strong>、<strong>F = kernel_size</strong>，以及<strong>S = stride</strong>。类似地，<strong>H_in</strong> 和 <strong>W_in</strong> 分别是 <strong>input_shape</strong> 元祖的第一个和第二个值。</p><p><strong>卷积层的深度</strong>始终为过滤器数量 <strong>K</strong>。</p><p>如果 <strong>padding = ‘same’</strong>，那么卷积层的空间维度如下：</p><ul><li>height = ceil(float(<strong>H_in</strong>) / float(<strong>S</strong>))</li><li>width = ceil(float(<strong>W_in</strong>) / float(<strong>S</strong>))</li></ul><p>如果 padding = ‘valid’，那么卷积层的空间维度如下:</p><ul><li>height = ceil(float(<strong>H_in</strong> - <strong>F</strong> + 1) / float(<strong>S</strong>))</li><li>width = ceil(float(<strong>W_in</strong> - <strong>F</strong> + 1) / float(<strong>S</strong>))</li></ul><h1 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from keras.models import Sequential</span><br><span class="line">from keras.layers import Conv2D</span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Conv2D(filters=32, kernel_size=3, strides=2, padding=&apos;same&apos;,</span><br><span class="line">    activation=&apos;relu&apos;, input_shape=(128, 128, 3)))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><h2 id="习题1"><a href="#习题1" class="headerlink" title="习题1"></a>习题1</h2><p>该卷积层有多少个参数？<br>A.902<br>B.306<br>C.896<br>D.1034</p><p>答案：(32 x 3 x 3 x 3) + 32 = 896</p><h2 id="习题2"><a href="#习题2" class="headerlink" title="习题2"></a>习题2</h2><p>卷积层的深度是多少？<br>A.3<br>B.16<br>C.32<br>D.64</p><p>答案：卷积层的深度始终等于过滤器的数量32。</p><h2 id="习题2-1"><a href="#习题2-1" class="headerlink" title="习题2"></a>习题2</h2><p>卷积层的宽度是多少？<br>A.3<br>B.16<br>C.32<br>D.64</p><p>答案：64</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>卷积神经的维度跟参数密切相关，Conv2D这个接口的参数很多，但是每个参数都有不同的作用。最后总结下通常用的比较多参数：</p><ul><li>filters - 过滤器数量。过滤器数量和每个过滤器的大小控制卷积层的行为。</li><li>kernel_size - 指定（方形）卷积窗口的高和宽的数字，可表示为数字或元组。</li></ul><p>你可能还需要调整其他可选参数：</p><ul><li>strides - 卷积 stride ，指过滤器滑过图片的数量。如果不指定任何值，则 strides 设为 1，可表示为数字或元组。</li><li>padding - 选项包括 ‘valid’ 和 ‘same’。如果不指定任何值，则 padding 设为 ‘valid’。</li><li>activation - 通常为 ‘relu’。如果未指定任何值，则不应用任何激活函数。强烈建议你向网络中的每个卷积层添加一个 ReLU 激活函数。</li></ul><p>在模型中将卷积层当做第一层级（出现在输入层之后）时，必须提供另一个 input_shape 参数：</p><ul><li>input_shape - 指定输入的高度、宽度和深度（按此顺序）的元组。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;神经网络的维度是个经常让人迷糊的概念，本文以卷积神经网络为例介绍怎么计算维度。文中的公式也可以作为知识点backup。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="https://www.seventech.top/categories/AI/"/>
    
    
      <category term="CNN" scheme="https://www.seventech.top/tags/CNN/"/>
    
  </entry>
  
</feed>
